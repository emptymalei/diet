{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation for DIETBox \u00a4 The Data scIEnce Toolbox (DIETBox). Install \u00a4 pip install dietbox This will leave out many dependencies. To install DIET together with other requirements, pip install \"dietbox[all]\" The extras options: all : everything visual : visualization related package aws : required if one needs AWS docs : required to build the docs","title":"Home"},{"location":"#documentation-for-dietbox","text":"The Data scIEnce Toolbox (DIETBox).","title":"Documentation for DIETBox"},{"location":"#install","text":"pip install dietbox This will leave out many dependencies. To install DIET together with other requirements, pip install \"dietbox[all]\" The extras options: all : everything visual : visualization related package aws : required if one needs AWS docs : required to build the docs","title":"Install"},{"location":"changelog/","text":"DIETBox Changelog \u00a4 2021-10-24, 0.1.0 \u00a4 Added docs. Removed modules that are already in haferml.","title":"Changelog"},{"location":"changelog/#dietbox-changelog","text":"","title":"DIETBox Changelog"},{"location":"changelog/#2021-10-24-010","text":"Added docs. Removed modules that are already in haferml.","title":"2021-10-24, 0.1.0"},{"location":"references/","text":"References \u00a4 In this section, we provide the references for the DIETBox codebase. For more details of how to use the package, please refer to tutorials . Warning This is still a WIP.","title":"Introduction"},{"location":"references/#references","text":"In this section, we provide the references for the DIETBox codebase. For more details of how to use the package, please refer to tutorials . Warning This is still a WIP.","title":"References"},{"location":"references/abtest/","text":"ABTest \u00a4","title":"abtest"},{"location":"references/abtest/#abtest","text":"","title":"ABTest"},{"location":"references/abtest/stats/","text":"ABTest - stats \u00a4 ABTestRatios \u00a4 This test uses the difference between the ratios $d=A_cr - B_cr$ as the signature. According to central limit theorem, we could approximate the distribution of d as a normal distribution. Null hypothesis: d = 0, sigma = pooled standard error Alternative: d !=0, sigma = pooled std error alt_distribution ( self ) \u00a4 Generate the distribution for the null hypothesis Source code in dietbox/abtest/stats.py def alt_distribution ( self ): \"\"\"Generate the distribution for the null hypothesis\"\"\" self . conversion_uplift () self . pooled_std_err () return scs . norm ( self . uplift , self . pld_std_err ) conversion_rate ( self ) \u00a4 Conversion rate for a specific group Source code in dietbox/abtest/stats.py def conversion_rate ( self ): \"\"\"Conversion rate for a specific group\"\"\" self . A_cr = cal_conversion_rate ( self . A_total , self . A_converted ) self . B_cr = cal_conversion_rate ( self . B_total , self . B_converted ) return self . A_cr , self . B_cr conversion_uplift ( self ) \u00a4 Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats.py def conversion_uplift ( self ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . uplift = cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ) return self . uplift null_distribution ( self ) \u00a4 Generate the distribution for the null hypothesis Source code in dietbox/abtest/stats.py def null_distribution ( self ): \"\"\"Generate the distribution for the null hypothesis\"\"\" self . pooled_std_err () return scs . norm ( 0 , self . pld_std_err ) pooled_probability ( self ) \u00a4 Pooled probability for two samples This is better used as an intermediate value. Source code in dietbox/abtest/stats.py def pooled_probability ( self ): \"\"\"Pooled probability for two samples This is better used as an intermediate value. \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . probability = cal_pooled_probability ( A_total , B_total , A_converted , B_converted ) return self . probability pooled_std_err ( self ) \u00a4 Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance Source code in dietbox/abtest/stats.py def pooled_std_err ( self ): \"\"\"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance \"\"\" self . pooled_probability () # Pooled standard error pp = self . probability self . pld_std_err = cal_pooled_std_err ( pp , self . A_total , self . B_total ) return self . pld_std_err report ( self , with_data = None , pipeline = None ) \u00a4 Run pipeline and generate report Source code in dietbox/abtest/stats.py def report ( self , with_data = None , pipeline = None ): \"\"\"Run pipeline and generate report\"\"\" if pipeline is None : pipeline = \"all\" if with_data is None : with_data = True all_pipes = [ meth for meth in dir ( self ) if callable ( getattr ( self , meth )) and \"__\" not in meth and meth != \"report\" ] for method in all_pipes : getattr ( self , method )() res = { \"kpi\" : { \"a\" : self . A_cr , \"b\" : self . B_cr }, \"std_err\" : { \"a\" : self . A_std_err , \"b\" : self . B_std_err }, \"pooled_std_err\" : self . pld_std_err , \"probability\" : self . probability , \"uplift\" : self . uplift , \"p_value\" : self . p , \"z_score\" : self . z , \"diff_std_err\" : self . diff_std_err , } if self . name : res [ \"name\" ] = self . name if with_data : res [ \"data\" ] = self . data return res standard_error ( self ) \u00a4 standard error Source code in dietbox/abtest/stats.py def standard_error ( self ): \"\"\"standard error\"\"\" self . A_std_err = cal_standard_error ( self . A_total , self . A_converted ) self . B_std_err = cal_standard_error ( self . B_total , self . B_converted ) return self . A_std_err , self . B_std_err ABTestRatiosNaive \u00a4 A naive AB Test ratios class conversion_rate ( self ) \u00a4 Conversion rate for a specific group Source code in dietbox/abtest/stats.py def conversion_rate ( self ): \"\"\"Conversion rate for a specific group\"\"\" self . A_cr = cal_conversion_rate ( self . A_total , self . A_converted ) self . B_cr = cal_conversion_rate ( self . B_total , self . B_converted ) return self . A_cr , self . B_cr conversion_uplift ( self ) \u00a4 Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats.py def conversion_uplift ( self ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . uplift = cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ) return self . uplift p_value ( self , test = None ) \u00a4 calculate p-value Source code in dietbox/abtest/stats.py def p_value ( self , test = None ): \"\"\"calculate p-value\"\"\" # self.conversion_rate() self . p = cal_p_value ( self . data ) return self . p pooled_probability ( self ) \u00a4 Pooled probability for two samples This is better used as an intermediate value. Source code in dietbox/abtest/stats.py def pooled_probability ( self ): \"\"\"Pooled probability for two samples This is better used as an intermediate value. \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . probability = cal_pooled_probability ( A_total , B_total , A_converted , B_converted ) return self . probability pooled_std_err ( self ) \u00a4 Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance Source code in dietbox/abtest/stats.py def pooled_std_err ( self ): \"\"\"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance \"\"\" self . pooled_probability () # Pooled standard error pp = self . probability self . pld_std_err = cal_pooled_std_err ( pp , self . A_total , self . B_total ) return self . pld_std_err report ( self , with_data = None , pipeline = None ) \u00a4 Run pipeline and Source code in dietbox/abtest/stats.py def report ( self , with_data = None , pipeline = None ): \"\"\"Run pipeline and\"\"\" if pipeline is None : pipeline = \"all\" if with_data is None : with_data = True all_pipes = [ meth for meth in dir ( self ) if callable ( getattr ( self , meth )) and \"__\" not in meth and meth != \"report\" ] for method in all_pipes : getattr ( self , method )() res = { \"kpi\" : { \"a\" : self . A_cr , \"b\" : self . B_cr }, \"std_err\" : { \"a\" : self . A_std_err , \"b\" : self . B_std_err }, \"pooled_std_err\" : self . pld_std_err , \"probability\" : self . probability , \"uplift\" : self . uplift , \"p_value\" : self . p , \"z_score\" : self . z , \"diff_std_err\" : self . diff_std_err , } if self . name : res [ \"name\" ] = self . name if with_data : res [ \"data\" ] = self . data return res standard_error ( self ) \u00a4 standard error Source code in dietbox/abtest/stats.py def standard_error ( self ): \"\"\"standard error\"\"\" self . A_std_err = cal_standard_error ( self . A_total , self . A_converted ) self . B_std_err = cal_standard_error ( self . B_total , self . B_converted ) return self . A_std_err , self . B_std_err z_score ( self , significance_level = None , two_tailed = None ) \u00a4 Calculate z-score Source code in dietbox/abtest/stats.py def z_score ( self , significance_level = None , two_tailed = None ): \"\"\"Calculate z-score\"\"\" if significance_level is None : significance_level = self . p if two_tailed is None : two_tailed = True self . z = cal_z_score ( significance_level , two_tailed ) return self . z ABTestSeries \u00a4 AB test of series data kpi ( self ) \u00a4 Conversion rate for a specific group Source code in dietbox/abtest/stats.py def kpi ( self ): \"\"\"Conversion rate for a specific group\"\"\" if self . kpi_method == \"count\" : self . A_kpi = cal_conversion_rate ( self . A_total , self . A_converted ) self . B_kpi = cal_conversion_rate ( self . B_total , self . B_converted ) else : self . A_kpi = self . A_converted self . B_kpi = self . B_converted return self . A_kpi , self . B_kpi kpi_uplift ( self ) \u00a4 Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats.py def kpi_uplift ( self ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" self . uplift = ( self . B_kpi - self . A_kpi ) / self . A_kpi return self . uplift p_value ( self ) \u00a4 calculate p-value Source code in dietbox/abtest/stats.py def p_value ( self ): \"\"\"calculate p-value\"\"\" self . kpi () self . p = cal_p_value ( self . data , test = \"mannwhitney\" ) return self . p report ( self , with_data = None , pipeline = None ) \u00a4 Run pipeline and Source code in dietbox/abtest/stats.py def report ( self , with_data = None , pipeline = None ): \"\"\"Run pipeline and\"\"\" if pipeline is None : pipeline = \"all\" if with_data is None : with_data = True all_pipes = [ meth for meth in dir ( self ) if callable ( getattr ( self , meth )) and \"__\" not in meth and meth != \"report\" ] for method in all_pipes : getattr ( self , method )() res = { \"kpi\" : { \"a\" : self . A_kpi , \"b\" : self . B_kpi }, \"std_err\" : { \"a\" : self . A_std_err , \"b\" : self . B_std_err }, \"uplift\" : self . uplift , \"p_value\" : self . p , } if self . name : res [ \"name\" ] = self . name if with_data : res [ \"data\" ] = self . data return res standard_error ( self ) \u00a4 standard error Source code in dietbox/abtest/stats.py def standard_error ( self ): \"\"\"standard error\"\"\" self . A_std_err = cal_standard_error ( self . A_total , self . A_converted ) self . B_std_err = cal_standard_error ( self . B_total , self . B_converted ) return self . A_std_err , self . B_std_err","title":"abtest.stats"},{"location":"references/abtest/stats/#abtest-stats","text":"","title":"ABTest - stats"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios","text":"This test uses the difference between the ratios $d=A_cr - B_cr$ as the signature. According to central limit theorem, we could approximate the distribution of d as a normal distribution. Null hypothesis: d = 0, sigma = pooled standard error Alternative: d !=0, sigma = pooled std error","title":"ABTestRatios"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.alt_distribution","text":"Generate the distribution for the null hypothesis Source code in dietbox/abtest/stats.py def alt_distribution ( self ): \"\"\"Generate the distribution for the null hypothesis\"\"\" self . conversion_uplift () self . pooled_std_err () return scs . norm ( self . uplift , self . pld_std_err )","title":"alt_distribution()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.conversion_rate","text":"Conversion rate for a specific group Source code in dietbox/abtest/stats.py def conversion_rate ( self ): \"\"\"Conversion rate for a specific group\"\"\" self . A_cr = cal_conversion_rate ( self . A_total , self . A_converted ) self . B_cr = cal_conversion_rate ( self . B_total , self . B_converted ) return self . A_cr , self . B_cr","title":"conversion_rate()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.conversion_uplift","text":"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats.py def conversion_uplift ( self ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . uplift = cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ) return self . uplift","title":"conversion_uplift()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.null_distribution","text":"Generate the distribution for the null hypothesis Source code in dietbox/abtest/stats.py def null_distribution ( self ): \"\"\"Generate the distribution for the null hypothesis\"\"\" self . pooled_std_err () return scs . norm ( 0 , self . pld_std_err )","title":"null_distribution()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.pooled_probability","text":"Pooled probability for two samples This is better used as an intermediate value. Source code in dietbox/abtest/stats.py def pooled_probability ( self ): \"\"\"Pooled probability for two samples This is better used as an intermediate value. \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . probability = cal_pooled_probability ( A_total , B_total , A_converted , B_converted ) return self . probability","title":"pooled_probability()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.pooled_std_err","text":"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance Source code in dietbox/abtest/stats.py def pooled_std_err ( self ): \"\"\"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance \"\"\" self . pooled_probability () # Pooled standard error pp = self . probability self . pld_std_err = cal_pooled_std_err ( pp , self . A_total , self . B_total ) return self . pld_std_err","title":"pooled_std_err()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.report","text":"Run pipeline and generate report Source code in dietbox/abtest/stats.py def report ( self , with_data = None , pipeline = None ): \"\"\"Run pipeline and generate report\"\"\" if pipeline is None : pipeline = \"all\" if with_data is None : with_data = True all_pipes = [ meth for meth in dir ( self ) if callable ( getattr ( self , meth )) and \"__\" not in meth and meth != \"report\" ] for method in all_pipes : getattr ( self , method )() res = { \"kpi\" : { \"a\" : self . A_cr , \"b\" : self . B_cr }, \"std_err\" : { \"a\" : self . A_std_err , \"b\" : self . B_std_err }, \"pooled_std_err\" : self . pld_std_err , \"probability\" : self . probability , \"uplift\" : self . uplift , \"p_value\" : self . p , \"z_score\" : self . z , \"diff_std_err\" : self . diff_std_err , } if self . name : res [ \"name\" ] = self . name if with_data : res [ \"data\" ] = self . data return res","title":"report()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatios.standard_error","text":"standard error Source code in dietbox/abtest/stats.py def standard_error ( self ): \"\"\"standard error\"\"\" self . A_std_err = cal_standard_error ( self . A_total , self . A_converted ) self . B_std_err = cal_standard_error ( self . B_total , self . B_converted ) return self . A_std_err , self . B_std_err","title":"standard_error()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive","text":"A naive AB Test ratios class","title":"ABTestRatiosNaive"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.conversion_rate","text":"Conversion rate for a specific group Source code in dietbox/abtest/stats.py def conversion_rate ( self ): \"\"\"Conversion rate for a specific group\"\"\" self . A_cr = cal_conversion_rate ( self . A_total , self . A_converted ) self . B_cr = cal_conversion_rate ( self . B_total , self . B_converted ) return self . A_cr , self . B_cr","title":"conversion_rate()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.conversion_uplift","text":"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats.py def conversion_uplift ( self ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . uplift = cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ) return self . uplift","title":"conversion_uplift()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.p_value","text":"calculate p-value Source code in dietbox/abtest/stats.py def p_value ( self , test = None ): \"\"\"calculate p-value\"\"\" # self.conversion_rate() self . p = cal_p_value ( self . data ) return self . p","title":"p_value()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.pooled_probability","text":"Pooled probability for two samples This is better used as an intermediate value. Source code in dietbox/abtest/stats.py def pooled_probability ( self ): \"\"\"Pooled probability for two samples This is better used as an intermediate value. \"\"\" A_total , B_total , A_converted , B_converted = ( self . A_total , self . B_total , self . A_converted , self . B_converted , ) self . probability = cal_pooled_probability ( A_total , B_total , A_converted , B_converted ) return self . probability","title":"pooled_probability()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.pooled_std_err","text":"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance Source code in dietbox/abtest/stats.py def pooled_std_err ( self ): \"\"\"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance \"\"\" self . pooled_probability () # Pooled standard error pp = self . probability self . pld_std_err = cal_pooled_std_err ( pp , self . A_total , self . B_total ) return self . pld_std_err","title":"pooled_std_err()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.report","text":"Run pipeline and Source code in dietbox/abtest/stats.py def report ( self , with_data = None , pipeline = None ): \"\"\"Run pipeline and\"\"\" if pipeline is None : pipeline = \"all\" if with_data is None : with_data = True all_pipes = [ meth for meth in dir ( self ) if callable ( getattr ( self , meth )) and \"__\" not in meth and meth != \"report\" ] for method in all_pipes : getattr ( self , method )() res = { \"kpi\" : { \"a\" : self . A_cr , \"b\" : self . B_cr }, \"std_err\" : { \"a\" : self . A_std_err , \"b\" : self . B_std_err }, \"pooled_std_err\" : self . pld_std_err , \"probability\" : self . probability , \"uplift\" : self . uplift , \"p_value\" : self . p , \"z_score\" : self . z , \"diff_std_err\" : self . diff_std_err , } if self . name : res [ \"name\" ] = self . name if with_data : res [ \"data\" ] = self . data return res","title":"report()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.standard_error","text":"standard error Source code in dietbox/abtest/stats.py def standard_error ( self ): \"\"\"standard error\"\"\" self . A_std_err = cal_standard_error ( self . A_total , self . A_converted ) self . B_std_err = cal_standard_error ( self . B_total , self . B_converted ) return self . A_std_err , self . B_std_err","title":"standard_error()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestRatiosNaive.z_score","text":"Calculate z-score Source code in dietbox/abtest/stats.py def z_score ( self , significance_level = None , two_tailed = None ): \"\"\"Calculate z-score\"\"\" if significance_level is None : significance_level = self . p if two_tailed is None : two_tailed = True self . z = cal_z_score ( significance_level , two_tailed ) return self . z","title":"z_score()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestSeries","text":"AB test of series data","title":"ABTestSeries"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestSeries.kpi","text":"Conversion rate for a specific group Source code in dietbox/abtest/stats.py def kpi ( self ): \"\"\"Conversion rate for a specific group\"\"\" if self . kpi_method == \"count\" : self . A_kpi = cal_conversion_rate ( self . A_total , self . A_converted ) self . B_kpi = cal_conversion_rate ( self . B_total , self . B_converted ) else : self . A_kpi = self . A_converted self . B_kpi = self . B_converted return self . A_kpi , self . B_kpi","title":"kpi()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestSeries.kpi_uplift","text":"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats.py def kpi_uplift ( self ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" self . uplift = ( self . B_kpi - self . A_kpi ) / self . A_kpi return self . uplift","title":"kpi_uplift()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestSeries.p_value","text":"calculate p-value Source code in dietbox/abtest/stats.py def p_value ( self ): \"\"\"calculate p-value\"\"\" self . kpi () self . p = cal_p_value ( self . data , test = \"mannwhitney\" ) return self . p","title":"p_value()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestSeries.report","text":"Run pipeline and Source code in dietbox/abtest/stats.py def report ( self , with_data = None , pipeline = None ): \"\"\"Run pipeline and\"\"\" if pipeline is None : pipeline = \"all\" if with_data is None : with_data = True all_pipes = [ meth for meth in dir ( self ) if callable ( getattr ( self , meth )) and \"__\" not in meth and meth != \"report\" ] for method in all_pipes : getattr ( self , method )() res = { \"kpi\" : { \"a\" : self . A_kpi , \"b\" : self . B_kpi }, \"std_err\" : { \"a\" : self . A_std_err , \"b\" : self . B_std_err }, \"uplift\" : self . uplift , \"p_value\" : self . p , } if self . name : res [ \"name\" ] = self . name if with_data : res [ \"data\" ] = self . data return res","title":"report()"},{"location":"references/abtest/stats/#dietbox.abtest.stats.ABTestSeries.standard_error","text":"standard error Source code in dietbox/abtest/stats.py def standard_error ( self ): \"\"\"standard error\"\"\" self . A_std_err = cal_standard_error ( self . A_total , self . A_converted ) self . B_std_err = cal_standard_error ( self . B_total , self . B_converted ) return self . A_std_err , self . B_std_err","title":"standard_error()"},{"location":"references/abtest/stats_util/","text":"ABTest - stats_util \u00a4 ab_dist ( stderr , d_hat = 0 , group_type = 'control' ) \u00a4 Function from https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f Distribution object depending on group type Examples: Parameters: stderr (float): pooled standard error of two independent samples d_hat (float): the mean difference between two independent samples group_type (string): 'control' and 'test' are supported Returns: dist (scipy.stats distribution object) Source code in dietbox/abtest/stats_util.py def ab_dist ( stderr , d_hat = 0 , group_type = \"control\" ): \"\"\"Function from https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f Distribution object depending on group type Examples: Parameters: stderr (float): pooled standard error of two independent samples d_hat (float): the mean difference between two independent samples group_type (string): 'control' and 'test' are supported Returns: dist (scipy.stats distribution object) \"\"\" if group_type == \"control\" : sample_mean = 0 elif group_type == \"test\" : sample_mean = d_hat # create a normal distribution which is dependent on mean and std dev dist = scs . norm ( sample_mean , stderr ) return dist cal_confidence_interval ( sample_mean = 0 , sample_std = 1 , sample_size = 1 , sig_level = 0.05 ) \u00a4 Confidence interval assuming normal distribution Based the z score. Reference: https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/confidence-interval/ Source code in dietbox/abtest/stats_util.py def cal_confidence_interval ( sample_mean = 0 , sample_std = 1 , sample_size = 1 , sig_level = 0.05 ): \"\"\"Confidence interval assuming normal distribution Based the z score. Reference: https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/confidence-interval/ \"\"\" z = cal_z_score ( sig_level ) left = sample_mean - z * sample_std / np . sqrt ( sample_size ) right = sample_mean + z * sample_std / np . sqrt ( sample_size ) return ( left , right ) cal_conversion_rate ( X_total , X_converted ) \u00a4 Conversion rate for a specific group Source code in dietbox/abtest/stats_util.py def cal_conversion_rate ( X_total , X_converted ): \"\"\"Conversion rate for a specific group\"\"\" X_cr = X_converted / X_total if X_total != 0 else 0 return X_cr cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ) \u00a4 Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats_util.py def cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" try : the_uplift = ( cal_conversion_rate ( B_total , B_converted ) - cal_conversion_rate ( A_total , A_converted ) ) / cal_conversion_rate ( A_total , A_converted ) except Exception as ee : raise Exception ( ee ) return the_uplift cal_difference_standard_error ( A_total , B_total , A_converted , B_converted ) \u00a4 Standard error of the difference for the AB test Source code in dietbox/abtest/stats_util.py def cal_difference_standard_error ( A_total , B_total , A_converted , B_converted ): \"\"\"Standard error of the difference for the AB test\"\"\" A_se = cal_standard_error ( A_total , A_converted ) B_se = cal_standard_error ( B_total , B_converted ) return np . sqrt ( A_se ** 2 + B_se ** 2 ) cal_p_value ( data , test = None ) \u00a4 p-value for the ratio test Source code in dietbox/abtest/stats_util.py def cal_p_value ( data , test = None ): \"\"\"p-value for the ratio test\"\"\" if test is None : test = \"binom\" if test == \"binom\" : A_total , B_total , A_converted , B_converted = ( data . get ( \"A_total\" ), data . get ( \"B_total\" ), data . get ( \"A_converted\" ), data . get ( \"B_converted\" ), ) if data . get ( \"A_cr\" ) and data . get ( \"B_cr\" ): p_A = data . get ( \"A_cr\" ) p_B = data . get ( \"B_cr\" ) else : p_A = cal_conversion_rate ( A_total , A_converted ) p_B = cal_conversion_rate ( B_total , B_converted ) return scs . binom_test ( B_converted , n = B_total , p = p_A , alternative = \"greater\" ) # return scs.binom(A_total, p_A).sf(B_total * p_B) elif test == \"mannwhitney\" : A_data , B_data = data . get ( \"A_series\" ), data . get ( \"B_series\" ) _ , test_mannwhitney_p_value = scs . mannwhitneyu ( x = A_data , y = B_data ) return test_mannwhitney_p_value cal_pooled_probability ( A_total , B_total , A_converted , B_converted ) \u00a4 Pooled probability for two samples This is better used as an intermediate value. Source code in dietbox/abtest/stats_util.py def cal_pooled_probability ( A_total , B_total , A_converted , B_converted ): \"\"\"Pooled probability for two samples This is better used as an intermediate value. \"\"\" probability = ( A_converted + B_converted ) / ( A_total + B_total ) return probability cal_pooled_std_err ( pooled_prob_inp , A_total , B_total ) \u00a4 Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance Source code in dietbox/abtest/stats_util.py def cal_pooled_std_err ( pooled_prob_inp , A_total , B_total ): \"\"\"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance \"\"\" # Pooled standard error pp = pooled_prob_inp pld_std_err = np . sqrt ( pp * ( 1 - pp ) * ( 1 / A_total + 1 / B_total )) return pld_std_err cal_standard_error ( X_total , X_converted ) \u00a4 standard error Source code in dietbox/abtest/stats_util.py def cal_standard_error ( X_total , X_converted ): \"\"\"standard error\"\"\" X_cr = cal_conversion_rate ( X_total , X_converted ) return np . sqrt ( X_cr * ( 1 - X_cr ) / X_total ) cal_z_score ( significance_level = None , two_tailed = None ) \u00a4 z score z score requires at least one parameter which is the significance level. By default, significance level for this function is assumed to be 0.05. It is better that the user read this Nature article before using this function: https://www.nature.com/articles/d41586-019-00857-9 Source code in dietbox/abtest/stats_util.py def cal_z_score ( significance_level = None , two_tailed = None ): \"\"\"z score z score requires at least one parameter which is the significance level. By default, significance level for this function is assumed to be 0.05. It is better that the user read this Nature article before using this function: https://www.nature.com/articles/d41586-019-00857-9 \"\"\" if significance_level is None : significance_level = 0.05 if two_tailed is None : two_tailed = True ## construct a z distribution z_distribution = scs . norm () if two_tailed : significance_level = significance_level / 2 no_rejection_level = 1 - significance_level else : no_rejection_level = 1 - significance_level ## generate z distribution z_score = z_distribution . ppf ( no_rejection_level ) return z_score","title":"abtest.stats_util"},{"location":"references/abtest/stats_util/#abtest-stats_util","text":"","title":"ABTest - stats_util"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.ab_dist","text":"Function from https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f Distribution object depending on group type Examples: Parameters: stderr (float): pooled standard error of two independent samples d_hat (float): the mean difference between two independent samples group_type (string): 'control' and 'test' are supported Returns: dist (scipy.stats distribution object) Source code in dietbox/abtest/stats_util.py def ab_dist ( stderr , d_hat = 0 , group_type = \"control\" ): \"\"\"Function from https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f Distribution object depending on group type Examples: Parameters: stderr (float): pooled standard error of two independent samples d_hat (float): the mean difference between two independent samples group_type (string): 'control' and 'test' are supported Returns: dist (scipy.stats distribution object) \"\"\" if group_type == \"control\" : sample_mean = 0 elif group_type == \"test\" : sample_mean = d_hat # create a normal distribution which is dependent on mean and std dev dist = scs . norm ( sample_mean , stderr ) return dist","title":"ab_dist()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_confidence_interval","text":"Confidence interval assuming normal distribution Based the z score. Reference: https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/confidence-interval/ Source code in dietbox/abtest/stats_util.py def cal_confidence_interval ( sample_mean = 0 , sample_std = 1 , sample_size = 1 , sig_level = 0.05 ): \"\"\"Confidence interval assuming normal distribution Based the z score. Reference: https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/confidence-interval/ \"\"\" z = cal_z_score ( sig_level ) left = sample_mean - z * sample_std / np . sqrt ( sample_size ) right = sample_mean + z * sample_std / np . sqrt ( sample_size ) return ( left , right )","title":"cal_confidence_interval()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_conversion_rate","text":"Conversion rate for a specific group Source code in dietbox/abtest/stats_util.py def cal_conversion_rate ( X_total , X_converted ): \"\"\"Conversion rate for a specific group\"\"\" X_cr = X_converted / X_total if X_total != 0 else 0 return X_cr","title":"cal_conversion_rate()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_conversion_uplift","text":"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups Source code in dietbox/abtest/stats_util.py def cal_conversion_uplift ( A_total , B_total , A_converted , B_converted ): \"\"\"Uplift in conversion rate Relative uplift is the relative conversion rate difference between the test groups \"\"\" try : the_uplift = ( cal_conversion_rate ( B_total , B_converted ) - cal_conversion_rate ( A_total , A_converted ) ) / cal_conversion_rate ( A_total , A_converted ) except Exception as ee : raise Exception ( ee ) return the_uplift","title":"cal_conversion_uplift()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_difference_standard_error","text":"Standard error of the difference for the AB test Source code in dietbox/abtest/stats_util.py def cal_difference_standard_error ( A_total , B_total , A_converted , B_converted ): \"\"\"Standard error of the difference for the AB test\"\"\" A_se = cal_standard_error ( A_total , A_converted ) B_se = cal_standard_error ( B_total , B_converted ) return np . sqrt ( A_se ** 2 + B_se ** 2 )","title":"cal_difference_standard_error()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_p_value","text":"p-value for the ratio test Source code in dietbox/abtest/stats_util.py def cal_p_value ( data , test = None ): \"\"\"p-value for the ratio test\"\"\" if test is None : test = \"binom\" if test == \"binom\" : A_total , B_total , A_converted , B_converted = ( data . get ( \"A_total\" ), data . get ( \"B_total\" ), data . get ( \"A_converted\" ), data . get ( \"B_converted\" ), ) if data . get ( \"A_cr\" ) and data . get ( \"B_cr\" ): p_A = data . get ( \"A_cr\" ) p_B = data . get ( \"B_cr\" ) else : p_A = cal_conversion_rate ( A_total , A_converted ) p_B = cal_conversion_rate ( B_total , B_converted ) return scs . binom_test ( B_converted , n = B_total , p = p_A , alternative = \"greater\" ) # return scs.binom(A_total, p_A).sf(B_total * p_B) elif test == \"mannwhitney\" : A_data , B_data = data . get ( \"A_series\" ), data . get ( \"B_series\" ) _ , test_mannwhitney_p_value = scs . mannwhitneyu ( x = A_data , y = B_data ) return test_mannwhitney_p_value","title":"cal_p_value()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_pooled_probability","text":"Pooled probability for two samples This is better used as an intermediate value. Source code in dietbox/abtest/stats_util.py def cal_pooled_probability ( A_total , B_total , A_converted , B_converted ): \"\"\"Pooled probability for two samples This is better used as an intermediate value. \"\"\" probability = ( A_converted + B_converted ) / ( A_total + B_total ) return probability","title":"cal_pooled_probability()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_pooled_std_err","text":"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance Source code in dietbox/abtest/stats_util.py def cal_pooled_std_err ( pooled_prob_inp , A_total , B_total ): \"\"\"Pooled standard error for two samples For more information about the definition, refer to wikipedia: https://en.wikipedia.org/wiki/Pooled_variance \"\"\" # Pooled standard error pp = pooled_prob_inp pld_std_err = np . sqrt ( pp * ( 1 - pp ) * ( 1 / A_total + 1 / B_total )) return pld_std_err","title":"cal_pooled_std_err()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_standard_error","text":"standard error Source code in dietbox/abtest/stats_util.py def cal_standard_error ( X_total , X_converted ): \"\"\"standard error\"\"\" X_cr = cal_conversion_rate ( X_total , X_converted ) return np . sqrt ( X_cr * ( 1 - X_cr ) / X_total )","title":"cal_standard_error()"},{"location":"references/abtest/stats_util/#dietbox.abtest.stats_util.cal_z_score","text":"z score z score requires at least one parameter which is the significance level. By default, significance level for this function is assumed to be 0.05. It is better that the user read this Nature article before using this function: https://www.nature.com/articles/d41586-019-00857-9 Source code in dietbox/abtest/stats_util.py def cal_z_score ( significance_level = None , two_tailed = None ): \"\"\"z score z score requires at least one parameter which is the significance level. By default, significance level for this function is assumed to be 0.05. It is better that the user read this Nature article before using this function: https://www.nature.com/articles/d41586-019-00857-9 \"\"\" if significance_level is None : significance_level = 0.05 if two_tailed is None : two_tailed = True ## construct a z distribution z_distribution = scs . norm () if two_tailed : significance_level = significance_level / 2 no_rejection_level = 1 - significance_level else : no_rejection_level = 1 - significance_level ## generate z distribution z_score = z_distribution . ppf ( no_rejection_level ) return z_score","title":"cal_z_score()"},{"location":"references/data/","text":"Data \u00a4 A few utilities for data related tasks.","title":"data"},{"location":"references/data/#data","text":"A few utilities for data related tasks.","title":"Data"},{"location":"references/data/analysis/","text":"Data - Analysis \u00a4","title":"data.analysis"},{"location":"references/data/analysis/#data-analysis","text":"","title":"Data - Analysis"},{"location":"references/data/analysis/descriptions/","text":"Data - Analysis - Descriptions \u00a4 count_column_values_within_ranges ( df_inp , column_name , bins = None ) \u00a4 Count the number of values of a specific column according to the define ranges. Parameters: Name Type Description Default df_inp pd.DataFrame pandas dataframe required column_name column name to be counted required bins list of values to be used as the ranges None Source code in dietbox/data/analysis/descriptions.py def count_column_values_within_ranges ( df_inp , column_name , bins = None ): \"\"\" Count the number of values of a specific column according to the define ranges. :param pd.DataFrame df_inp: pandas dataframe :param column_name: column name to be counted :param bins: list of values to be used as the ranges \"\"\" if bins is None : bins = np . arange ( 0 , 7000 , 100 ) all_values = df_inp [ column_name ] . values # TODO: check type then convert all_values = all_values . astype ( np . float ) try : df_inp . loc [:, \"count\" ] = pd . cut ( df_inp [ column_name ] . astype ( float ), bins ) except Exception as e : print ( e ) print ( \"pd.cut produces\" , pd . cut ( df_inp [ column_name ] . astype ( float ), bins )) raise Exception ( \"Can not set cut to column\" ) df_counting = df_inp [ \"count\" ] . value_counts () . sort_index () df_counting = df_counting . to_frame () . reset_index () df_counting . columns = [ \"prices\" , \"count\" ] df_counting . loc [:, \"percent\" ] = df_counting [ \"count\" ] / df_counting [ \"count\" ] . sum () couting_data = { \"price\" : bins [: - 1 ], \"count\" : df_counting [ \"count\" ] . values , \"percent\" : df_counting [ \"percent\" ] . values , \"all_prices\" : all_values , } return couting_data count_column_values_within_ranges_two_levels_deep ( df_inp , first_groupby_column_name , second_groupby_column_name , count_column_name , bins = None ) \u00a4 Count column values within ranges, but groupby twice in dataframe Source code in dietbox/data/analysis/descriptions.py def count_column_values_within_ranges_two_levels_deep ( df_inp , first_groupby_column_name , second_groupby_column_name , count_column_name , bins = None , ): \"\"\" Count column values within ranges, but groupby twice in dataframe \"\"\" if bins is None : logger . warning ( \"No bins specified, will use a default range 0-10000\" ) bins = np . arange ( 0 , 10000 , 100 ) df_first_groups = df_inp . groupby ( first_groupby_column_name ) list_of_first_groups = [] return_data = {} for first_groups_key , one_df_of_first_groups in df_first_groups : list_of_first_groups . append ( first_groups_key ) counting_data_of_one_group = {} df_second_level_groups = one_df_of_first_groups . groupby ( second_groupby_column_name ) for second_groups_key , one_df_of_second_groups in df_second_level_groups : counting_data_of_one_group [ second_groups_key ] = count_column_values_within_ranges ( one_df_of_second_groups , count_column_name , bins ) return_data [ first_groups_key ] = counting_data_of_one_group return return_data","title":"data.analysis.description"},{"location":"references/data/analysis/descriptions/#data-analysis-descriptions","text":"","title":"Data - Analysis - Descriptions"},{"location":"references/data/analysis/descriptions/#dietbox.data.analysis.descriptions.count_column_values_within_ranges","text":"Count the number of values of a specific column according to the define ranges. Parameters: Name Type Description Default df_inp pd.DataFrame pandas dataframe required column_name column name to be counted required bins list of values to be used as the ranges None Source code in dietbox/data/analysis/descriptions.py def count_column_values_within_ranges ( df_inp , column_name , bins = None ): \"\"\" Count the number of values of a specific column according to the define ranges. :param pd.DataFrame df_inp: pandas dataframe :param column_name: column name to be counted :param bins: list of values to be used as the ranges \"\"\" if bins is None : bins = np . arange ( 0 , 7000 , 100 ) all_values = df_inp [ column_name ] . values # TODO: check type then convert all_values = all_values . astype ( np . float ) try : df_inp . loc [:, \"count\" ] = pd . cut ( df_inp [ column_name ] . astype ( float ), bins ) except Exception as e : print ( e ) print ( \"pd.cut produces\" , pd . cut ( df_inp [ column_name ] . astype ( float ), bins )) raise Exception ( \"Can not set cut to column\" ) df_counting = df_inp [ \"count\" ] . value_counts () . sort_index () df_counting = df_counting . to_frame () . reset_index () df_counting . columns = [ \"prices\" , \"count\" ] df_counting . loc [:, \"percent\" ] = df_counting [ \"count\" ] / df_counting [ \"count\" ] . sum () couting_data = { \"price\" : bins [: - 1 ], \"count\" : df_counting [ \"count\" ] . values , \"percent\" : df_counting [ \"percent\" ] . values , \"all_prices\" : all_values , } return couting_data","title":"count_column_values_within_ranges()"},{"location":"references/data/analysis/descriptions/#dietbox.data.analysis.descriptions.count_column_values_within_ranges_two_levels_deep","text":"Count column values within ranges, but groupby twice in dataframe Source code in dietbox/data/analysis/descriptions.py def count_column_values_within_ranges_two_levels_deep ( df_inp , first_groupby_column_name , second_groupby_column_name , count_column_name , bins = None , ): \"\"\" Count column values within ranges, but groupby twice in dataframe \"\"\" if bins is None : logger . warning ( \"No bins specified, will use a default range 0-10000\" ) bins = np . arange ( 0 , 10000 , 100 ) df_first_groups = df_inp . groupby ( first_groupby_column_name ) list_of_first_groups = [] return_data = {} for first_groups_key , one_df_of_first_groups in df_first_groups : list_of_first_groups . append ( first_groups_key ) counting_data_of_one_group = {} df_second_level_groups = one_df_of_first_groups . groupby ( second_groupby_column_name ) for second_groups_key , one_df_of_second_groups in df_second_level_groups : counting_data_of_one_group [ second_groups_key ] = count_column_values_within_ranges ( one_df_of_second_groups , count_column_name , bins ) return_data [ first_groups_key ] = counting_data_of_one_group return return_data","title":"count_column_values_within_ranges_two_levels_deep()"},{"location":"references/data/extraction/","text":"Data - Extraction \u00a4","title":"data.extraction"},{"location":"references/data/extraction/#data-extraction","text":"","title":"Data - Extraction"},{"location":"references/data/extraction/web/","text":"Data - Analysis - Web \u00a4 get_page_content ( link , session = None , session_query_configs = None , method = 'GET' , data = None ) \u00a4 Download page and save content Parameters: Name Type Description Default headers dict, optional header information such as useragent, defaults to random user agent from get_random_user_agent required Source code in dietbox/data/extraction/web.py def get_page_content ( link , session = None , session_query_configs = None , method = \"GET\" , data = None ): \"\"\"Download page and save content :param headers: header information such as useragent, defaults to random user agent from get_random_user_agent :type headers: dict, optional \"\"\" if not session_query_configs : session_query_configs = get_session_query_configs () if not session : session = get_session ( retry_params = None , session = None , ) if method == \"GET\" : content = session . get ( link , ** session_query_configs ) elif method == \"POST\" : if data is None : data = {} content = session . post ( link , data = data , ** session_query_configs ) status = content . status_code return { \"status\" : status , \"content\" : content } get_random_user_agent ( browsers = None ) \u00a4 get_random_user_agent returns a random user agent. We provide two predefined browers, chrome and firefox. Parameters: Name Type Description Default browsers list, optional which brower to be used, defaults to [\"chrome\", \"firefox\"] None Returns: Type Description dict dictionary for requests module to consude as {'User-Agent': \"blabla\"} Source code in dietbox/data/extraction/web.py def get_random_user_agent ( browsers = None ): \"\"\" get_random_user_agent returns a random user agent. We provide two predefined browers, chrome and firefox. :param browsers: which brower to be used, defaults to [\"chrome\", \"firefox\"] :type browsers: list, optional :return: dictionary for requests module to consude as {'User-Agent': \"blabla\"} :rtype: dict \"\"\" if browsers is None : browsers = [ \"chrome\" , \"firefox\" ] if isinstance ( browsers , str ): browsers = [ browsers ] chrome_user_agents = [ \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\" , \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\" , ] firefox_user_agents = [ \"Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)\" , \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)\" , \"Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)\" , \"Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)\" , \"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)\" , \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\" , ] user_agents_dict = { \"chrome\" : chrome_user_agents , \"firefox\" : firefox_user_agents } # error if specified browser is not in the list if set ( browsers ) - set ( user_agents_dict . keys ()): logger . error ( f \"Unknown browser: { set ( browsers ) - set ( user_agents_dict . keys ()) } \" ) user_agent_list = sum ([ user_agents_dict [ browser ] for browser in browsers ], []) return { \"User-Agent\" : random . choice ( user_agent_list )} get_session ( retry_params = None , session = None ) \u00a4 get_session prepares a session object. Parameters: Name Type Description Default retry_params dict, optional the rules to retry, defaults to {\"retries\": 5, \"backoff_factor\": 0.3, \"status_forcelist\": (500, 502, 504)} None session [type], optional [description], defaults to None None Source code in dietbox/data/extraction/web.py def get_session ( retry_params = None , session = None , ): \"\"\" get_session prepares a session object. :param retry_params: the rules to retry, defaults to {\"retries\": 5, \"backoff_factor\": 0.3, \"status_forcelist\": (500, 502, 504)} :type retry_params: dict, optional :param session: [description], defaults to None :type session: [type], optional \"\"\" if retry_params is None : retry_params = { \"retries\" : 5 , \"backoff_factor\" : 0.3 , \"status_forcelist\" : ( 500 , 502 , 504 ), } if session is None : session = requests . Session () retry = Retry ( total = retry_params . get ( \"retries\" ), read = retry_params . get ( \"retries\" ), connect = retry_params . get ( \"retries\" ), backoff_factor = retry_params . get ( \"backoff_factor\" ), status_forcelist = retry_params . get ( \"status_forcelist\" ), ) adapter = HTTPAdapter ( max_retries = retry ) session . mount ( \"http://\" , adapter ) session . mount ( \"https://\" , adapter ) return session get_session_query_configs ( headers = None , timeout = None , proxies = None , cookies = None ) \u00a4 get_session_query_configs creates a session config dictionary for session to use. These are the keyword arguments of the session get or post methods. Proxies can be set by providing a dictionary of the form { 'http' : some super_proxy_url , 'https' : some super_proxy_url , } Parameters: Name Type Description Default headers dict, optional header of the method such as use agent, defaults to random user agent from get_random_user_agent None timeout tuple, optional timeout strategy, defaults to (5, 14) None proxies dict, optional proxy configs, defaults to {} None cookies dict, optional cookie configs, defaults to {\"language\": \"en\"} None Returns: Type Description dict dictionary of session configs for session methods, e.g., get, to use. Source code in dietbox/data/extraction/web.py def get_session_query_configs ( headers = None , timeout = None , proxies = None , cookies = None , ): \"\"\" get_session_query_configs creates a session config dictionary for session to use. These are the keyword arguments of the session get or post methods. Proxies can be set by providing a dictionary of the form ```python { 'http': some super_proxy_url, 'https': some super_proxy_url, } ``` :param headers: header of the method such as use agent, defaults to random user agent from get_random_user_agent :type headers: dict, optional :param timeout: timeout strategy, defaults to (5, 14) :type timeout: tuple, optional :param proxies: proxy configs, defaults to {} :type proxies: dict, optional :param cookies: cookie configs, defaults to {\"language\": \"en\"} :type cookies: dict, optional :return: dictionary of session configs for session methods, e.g., get, to use. :rtype: dict \"\"\" if cookies is None : cookies = { \"language\" : \"en\" } if headers is None : headers = get_random_user_agent () if timeout is None : timeout = ( 5 , 14 ) if proxies is None : proxies = {} return dict ( headers = headers , proxies = proxies , cookies = cookies )","title":"data.extraction.web"},{"location":"references/data/extraction/web/#data-analysis-web","text":"","title":"Data - Analysis - Web"},{"location":"references/data/extraction/web/#dietbox.data.extraction.web.get_page_content","text":"Download page and save content Parameters: Name Type Description Default headers dict, optional header information such as useragent, defaults to random user agent from get_random_user_agent required Source code in dietbox/data/extraction/web.py def get_page_content ( link , session = None , session_query_configs = None , method = \"GET\" , data = None ): \"\"\"Download page and save content :param headers: header information such as useragent, defaults to random user agent from get_random_user_agent :type headers: dict, optional \"\"\" if not session_query_configs : session_query_configs = get_session_query_configs () if not session : session = get_session ( retry_params = None , session = None , ) if method == \"GET\" : content = session . get ( link , ** session_query_configs ) elif method == \"POST\" : if data is None : data = {} content = session . post ( link , data = data , ** session_query_configs ) status = content . status_code return { \"status\" : status , \"content\" : content }","title":"get_page_content()"},{"location":"references/data/extraction/web/#dietbox.data.extraction.web.get_random_user_agent","text":"get_random_user_agent returns a random user agent. We provide two predefined browers, chrome and firefox. Parameters: Name Type Description Default browsers list, optional which brower to be used, defaults to [\"chrome\", \"firefox\"] None Returns: Type Description dict dictionary for requests module to consude as {'User-Agent': \"blabla\"} Source code in dietbox/data/extraction/web.py def get_random_user_agent ( browsers = None ): \"\"\" get_random_user_agent returns a random user agent. We provide two predefined browers, chrome and firefox. :param browsers: which brower to be used, defaults to [\"chrome\", \"firefox\"] :type browsers: list, optional :return: dictionary for requests module to consude as {'User-Agent': \"blabla\"} :rtype: dict \"\"\" if browsers is None : browsers = [ \"chrome\" , \"firefox\" ] if isinstance ( browsers , str ): browsers = [ browsers ] chrome_user_agents = [ \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\" , \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\" , ] firefox_user_agents = [ \"Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)\" , \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)\" , \"Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)\" , \"Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\" , \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko\" , \"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)\" , \"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)\" , \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\" , ] user_agents_dict = { \"chrome\" : chrome_user_agents , \"firefox\" : firefox_user_agents } # error if specified browser is not in the list if set ( browsers ) - set ( user_agents_dict . keys ()): logger . error ( f \"Unknown browser: { set ( browsers ) - set ( user_agents_dict . keys ()) } \" ) user_agent_list = sum ([ user_agents_dict [ browser ] for browser in browsers ], []) return { \"User-Agent\" : random . choice ( user_agent_list )}","title":"get_random_user_agent()"},{"location":"references/data/extraction/web/#dietbox.data.extraction.web.get_session","text":"get_session prepares a session object. Parameters: Name Type Description Default retry_params dict, optional the rules to retry, defaults to {\"retries\": 5, \"backoff_factor\": 0.3, \"status_forcelist\": (500, 502, 504)} None session [type], optional [description], defaults to None None Source code in dietbox/data/extraction/web.py def get_session ( retry_params = None , session = None , ): \"\"\" get_session prepares a session object. :param retry_params: the rules to retry, defaults to {\"retries\": 5, \"backoff_factor\": 0.3, \"status_forcelist\": (500, 502, 504)} :type retry_params: dict, optional :param session: [description], defaults to None :type session: [type], optional \"\"\" if retry_params is None : retry_params = { \"retries\" : 5 , \"backoff_factor\" : 0.3 , \"status_forcelist\" : ( 500 , 502 , 504 ), } if session is None : session = requests . Session () retry = Retry ( total = retry_params . get ( \"retries\" ), read = retry_params . get ( \"retries\" ), connect = retry_params . get ( \"retries\" ), backoff_factor = retry_params . get ( \"backoff_factor\" ), status_forcelist = retry_params . get ( \"status_forcelist\" ), ) adapter = HTTPAdapter ( max_retries = retry ) session . mount ( \"http://\" , adapter ) session . mount ( \"https://\" , adapter ) return session","title":"get_session()"},{"location":"references/data/extraction/web/#dietbox.data.extraction.web.get_session_query_configs","text":"get_session_query_configs creates a session config dictionary for session to use. These are the keyword arguments of the session get or post methods. Proxies can be set by providing a dictionary of the form { 'http' : some super_proxy_url , 'https' : some super_proxy_url , } Parameters: Name Type Description Default headers dict, optional header of the method such as use agent, defaults to random user agent from get_random_user_agent None timeout tuple, optional timeout strategy, defaults to (5, 14) None proxies dict, optional proxy configs, defaults to {} None cookies dict, optional cookie configs, defaults to {\"language\": \"en\"} None Returns: Type Description dict dictionary of session configs for session methods, e.g., get, to use. Source code in dietbox/data/extraction/web.py def get_session_query_configs ( headers = None , timeout = None , proxies = None , cookies = None , ): \"\"\" get_session_query_configs creates a session config dictionary for session to use. These are the keyword arguments of the session get or post methods. Proxies can be set by providing a dictionary of the form ```python { 'http': some super_proxy_url, 'https': some super_proxy_url, } ``` :param headers: header of the method such as use agent, defaults to random user agent from get_random_user_agent :type headers: dict, optional :param timeout: timeout strategy, defaults to (5, 14) :type timeout: tuple, optional :param proxies: proxy configs, defaults to {} :type proxies: dict, optional :param cookies: cookie configs, defaults to {\"language\": \"en\"} :type cookies: dict, optional :return: dictionary of session configs for session methods, e.g., get, to use. :rtype: dict \"\"\" if cookies is None : cookies = { \"language\" : \"en\" } if headers is None : headers = get_random_user_agent () if timeout is None : timeout = ( 5 , 14 ) if proxies is None : proxies = {} return dict ( headers = headers , proxies = proxies , cookies = cookies )","title":"get_session_query_configs()"},{"location":"references/data/sync/","text":"Data - Sync \u00a4 dietbox.data.sync module contains the utilities for syncing artifacts.","title":"data.sync"},{"location":"references/data/sync/#data-sync","text":"dietbox.data.sync module contains the utilities for syncing artifacts.","title":"Data - Sync"},{"location":"references/data/sync/aws/","text":"Data - Sync - AWS \u00a4 aws_cli ( * cmd ) \u00a4 aws_cli invokes the aws cli processes in python to execute awscli commands. .. warning:: This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. Parameters: Name Type Description Default *cmd tuple of awscli command. .. admonition:: Examples :class: info AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) .. admonition:: References :class: info This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 () Source code in dietbox/data/sync/aws.py def aws_cli ( * cmd ): \"\"\" aws_cli invokes the aws cli processes in python to execute awscli commands. .. warning:: This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. :param *cmd: tuple of awscli command. .. admonition:: Examples :class: info AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) .. admonition:: References :class: info This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 \"\"\" old_env = dict ( os . environ ) try : # Set up environment env = os . environ . copy () env [ \"LC_CTYPE\" ] = \"en_US.UTF\" os . environ . update ( env ) # Run awscli in the same process exit_code = create_clidriver () . main ( * cmd ) # Deal with problems if exit_code > 0 : raise RuntimeError ( f \"AWS CLI exited with code { exit_code } \" ) finally : os . environ . clear () os . environ . update ( old_env )","title":"data.sync.aws"},{"location":"references/data/sync/aws/#data-sync-aws","text":"","title":"Data - Sync - AWS"},{"location":"references/data/sync/aws/#dietbox.data.sync.aws.aws_cli","text":"aws_cli invokes the aws cli processes in python to execute awscli commands. .. warning:: This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. Parameters: Name Type Description Default *cmd tuple of awscli command. .. admonition:: Examples :class: info AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) .. admonition:: References :class: info This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 () Source code in dietbox/data/sync/aws.py def aws_cli ( * cmd ): \"\"\" aws_cli invokes the aws cli processes in python to execute awscli commands. .. warning:: This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. :param *cmd: tuple of awscli command. .. admonition:: Examples :class: info AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) .. admonition:: References :class: info This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 \"\"\" old_env = dict ( os . environ ) try : # Set up environment env = os . environ . copy () env [ \"LC_CTYPE\" ] = \"en_US.UTF\" os . environ . update ( env ) # Run awscli in the same process exit_code = create_clidriver () . main ( * cmd ) # Deal with problems if exit_code > 0 : raise RuntimeError ( f \"AWS CLI exited with code { exit_code } \" ) finally : os . environ . clear () os . environ . update ( old_env )","title":"aws_cli()"},{"location":"references/data/sync/local/","text":"Data - Sync - Local \u00a4 LocalStorage \u00a4 A model for local storage is_in_storage ( self , record_identifier , record_identifier_lookup_paths ) \u00a4 Check if the record is already in storage Source code in dietbox/data/sync/local.py def is_in_storage ( self , record_identifier , record_identifier_lookup_paths ): \"\"\"Check if the record is already in storage\"\"\" if isinstance ( record_identifier_lookup_paths , str ): record_identifier_lookup_paths = [ record_identifier_lookup_paths ] if not isinstance ( record_identifier , str ): logger . warning ( \"Input data is not string\" ) try : record_identifier = str ( record_identifier ) except Exception as ee : logger . error ( f \"Could not convert input { record_identifier } to string! { ee } \" ) return { \"exists\" : False , \"record\" : None } record_identifier = record_identifier . lower () if not self . records : all_existing_records = self . load_records () all_existing_records = self . records for record in all_existing_records : for record_identifier_lookup_path in record_identifier_lookup_paths : record_company = _get_value_in_dict_recursively ( record , record_identifier_lookup_path ) if record_company : record_company = record_company . lower () if record_identifier == record_company : return { \"exists\" : True , \"record\" : record } return { \"exists\" : False , \"record\" : None } load_records ( self , keep_in_memory = True ) \u00a4 Load records for target Source code in dietbox/data/sync/local.py def load_records ( self , keep_in_memory = True ): \"\"\"Load records for target\"\"\" all_records = load_records ( self . target ) if keep_in_memory : self . records = all_records return all_records save_records ( self , record ) \u00a4 Save records in target Source code in dietbox/data/sync/local.py def save_records ( self , record ): \"\"\"Save records in target\"\"\" company = record . get ( \"company\" ) if self . is_in_storage ( company ) . get ( \"exists\" ): logger . debug ( f \" { company } already exists! No need to save again!\" ) save_records ( record , self . target , is_flush = True ) cache_dataframe ( dataframe , file , engine = None ) \u00a4 Write dataframe to a line-delineated json file. .. warning:: pandas engine doesn't respect the date encoder define since it has its own. Parameters: Name Type Description Default dataframe inut pandas dataframe required file str path of file to be written to required engine we have two engines to convert the data to json, pandas and json. None Returns: Type Description list if the engine is json, the converted json records are returned for inspections Source code in dietbox/data/sync/local.py def cache_dataframe ( dataframe , file , engine = None ): \"\"\" Write dataframe to a line-delineated json file. .. warning:: pandas engine doesn't respect the date encoder define since it has its own. :param dataframe: inut pandas dataframe :param str file: path of file to be written to :param engine: we have two engines to convert the data to json, pandas and json. :return: if the engine is json, the converted json records are returned for inspections :rtype: list \"\"\" if engine is None : engine = \"json\" if os . path . isfile ( file ): logger . error ( \"File ' {} ' exists, overwriting...\" . format ( file )) if engine == \"json\" : res = [] with open ( file , \"w\" ) as f : for _ , row in dataframe . iterrows (): row_dict = row . to_dict () logger . debug ( \"cache_dataframe::\" , row_dict ) res . append ( row_dict ) f . write ( json . dumps ( row . to_dict (), default = isoencode , ignore_nan = True ) + \" \\n \" ) return res elif engine == \"pandas\" : dataframe . to_json ( file , orient = \"records\" , lines = True , date_format = \"iso\" , default_handler = isoencode , ) else : raise Exception ( f \"No engine defined for { engine } \" ) load_records ( data_path_inp ) \u00a4 Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. Parameters: Name Type Description Default data_path_inp data file path required Returns: Type Description list of dicts Source code in dietbox/data/sync/local.py def load_records ( data_path_inp ): \"\"\"Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_path_inp: data file path :return: list of dicts \"\"\" data = [] with open ( data_path_inp , \"r\" ) as fp : for line in fp : line = line . replace ( \"null\" , ' \"None\" ' ) try : line_data = json . loads ( line . strip ()) except Exception as ee : logger . warning ( \"could not load \" , line , \" \\n \" , ee ) data . append ( line_data ) return data save_records ( data_inp , output , is_flush = None , write_mode = None ) \u00a4 Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :is_flush: whether to flush data to file for each row written to file Parameters: Name Type Description Default data_inp dict or list of dict to be saved required output path to output file required Returns: Type Description None Source code in dietbox/data/sync/local.py def save_records ( data_inp , output , is_flush = None , write_mode = None ): \"\"\"Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_inp: dict or list of dict to be saved :param output: path to output file :is_flush: whether to flush data to file for each row written to file :return: None \"\"\" if write_mode is None : write_mode = \"a+\" if is_flush is None : is_flush = False if isinstance ( data_inp , list ): data = data_inp elif isinstance ( data_inp , dict ): data = [ data_inp ] else : raise Exception ( \"Input data is neither list nor dict: {} \" . format ( data_inp )) try : with open ( output , write_mode ) as fp : for i in data : json . dump ( i , fp ) fp . write ( \" \\n \" ) if is_flush : fp . flush () except Exception as ee : raise Exception ( \"Could not load data to file: {} \" . format ( ee ))","title":"data.sync.local"},{"location":"references/data/sync/local/#data-sync-local","text":"","title":"Data - Sync - Local"},{"location":"references/data/sync/local/#dietbox.data.sync.local.LocalStorage","text":"A model for local storage","title":"LocalStorage"},{"location":"references/data/sync/local/#dietbox.data.sync.local.LocalStorage.is_in_storage","text":"Check if the record is already in storage Source code in dietbox/data/sync/local.py def is_in_storage ( self , record_identifier , record_identifier_lookup_paths ): \"\"\"Check if the record is already in storage\"\"\" if isinstance ( record_identifier_lookup_paths , str ): record_identifier_lookup_paths = [ record_identifier_lookup_paths ] if not isinstance ( record_identifier , str ): logger . warning ( \"Input data is not string\" ) try : record_identifier = str ( record_identifier ) except Exception as ee : logger . error ( f \"Could not convert input { record_identifier } to string! { ee } \" ) return { \"exists\" : False , \"record\" : None } record_identifier = record_identifier . lower () if not self . records : all_existing_records = self . load_records () all_existing_records = self . records for record in all_existing_records : for record_identifier_lookup_path in record_identifier_lookup_paths : record_company = _get_value_in_dict_recursively ( record , record_identifier_lookup_path ) if record_company : record_company = record_company . lower () if record_identifier == record_company : return { \"exists\" : True , \"record\" : record } return { \"exists\" : False , \"record\" : None }","title":"is_in_storage()"},{"location":"references/data/sync/local/#dietbox.data.sync.local.LocalStorage.load_records","text":"Load records for target Source code in dietbox/data/sync/local.py def load_records ( self , keep_in_memory = True ): \"\"\"Load records for target\"\"\" all_records = load_records ( self . target ) if keep_in_memory : self . records = all_records return all_records","title":"load_records()"},{"location":"references/data/sync/local/#dietbox.data.sync.local.LocalStorage.save_records","text":"Save records in target Source code in dietbox/data/sync/local.py def save_records ( self , record ): \"\"\"Save records in target\"\"\" company = record . get ( \"company\" ) if self . is_in_storage ( company ) . get ( \"exists\" ): logger . debug ( f \" { company } already exists! No need to save again!\" ) save_records ( record , self . target , is_flush = True )","title":"save_records()"},{"location":"references/data/sync/local/#dietbox.data.sync.local.cache_dataframe","text":"Write dataframe to a line-delineated json file. .. warning:: pandas engine doesn't respect the date encoder define since it has its own. Parameters: Name Type Description Default dataframe inut pandas dataframe required file str path of file to be written to required engine we have two engines to convert the data to json, pandas and json. None Returns: Type Description list if the engine is json, the converted json records are returned for inspections Source code in dietbox/data/sync/local.py def cache_dataframe ( dataframe , file , engine = None ): \"\"\" Write dataframe to a line-delineated json file. .. warning:: pandas engine doesn't respect the date encoder define since it has its own. :param dataframe: inut pandas dataframe :param str file: path of file to be written to :param engine: we have two engines to convert the data to json, pandas and json. :return: if the engine is json, the converted json records are returned for inspections :rtype: list \"\"\" if engine is None : engine = \"json\" if os . path . isfile ( file ): logger . error ( \"File ' {} ' exists, overwriting...\" . format ( file )) if engine == \"json\" : res = [] with open ( file , \"w\" ) as f : for _ , row in dataframe . iterrows (): row_dict = row . to_dict () logger . debug ( \"cache_dataframe::\" , row_dict ) res . append ( row_dict ) f . write ( json . dumps ( row . to_dict (), default = isoencode , ignore_nan = True ) + \" \\n \" ) return res elif engine == \"pandas\" : dataframe . to_json ( file , orient = \"records\" , lines = True , date_format = \"iso\" , default_handler = isoencode , ) else : raise Exception ( f \"No engine defined for { engine } \" )","title":"cache_dataframe()"},{"location":"references/data/sync/local/#dietbox.data.sync.local.load_records","text":"Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. Parameters: Name Type Description Default data_path_inp data file path required Returns: Type Description list of dicts Source code in dietbox/data/sync/local.py def load_records ( data_path_inp ): \"\"\"Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_path_inp: data file path :return: list of dicts \"\"\" data = [] with open ( data_path_inp , \"r\" ) as fp : for line in fp : line = line . replace ( \"null\" , ' \"None\" ' ) try : line_data = json . loads ( line . strip ()) except Exception as ee : logger . warning ( \"could not load \" , line , \" \\n \" , ee ) data . append ( line_data ) return data","title":"load_records()"},{"location":"references/data/sync/local/#dietbox.data.sync.local.save_records","text":"Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :is_flush: whether to flush data to file for each row written to file Parameters: Name Type Description Default data_inp dict or list of dict to be saved required output path to output file required Returns: Type Description None Source code in dietbox/data/sync/local.py def save_records ( data_inp , output , is_flush = None , write_mode = None ): \"\"\"Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_inp: dict or list of dict to be saved :param output: path to output file :is_flush: whether to flush data to file for each row written to file :return: None \"\"\" if write_mode is None : write_mode = \"a+\" if is_flush is None : is_flush = False if isinstance ( data_inp , list ): data = data_inp elif isinstance ( data_inp , dict ): data = [ data_inp ] else : raise Exception ( \"Input data is neither list nor dict: {} \" . format ( data_inp )) try : with open ( output , write_mode ) as fp : for i in data : json . dump ( i , fp ) fp . write ( \" \\n \" ) if is_flush : fp . flush () except Exception as ee : raise Exception ( \"Could not load data to file: {} \" . format ( ee ))","title":"save_records()"},{"location":"references/data/wrangling/","text":"Data - Wrangling \u00a4 dietbox.data.wrangling module contains the utilities for syncing artifacts.","title":"data.wrangling"},{"location":"references/data/wrangling/#data-wrangling","text":"dietbox.data.wrangling module contains the utilities for syncing artifacts.","title":"Data - Wrangling"},{"location":"references/data/wrangling/datetime/","text":"Data - Wrangling - datetime \u00a4 convert_to_datetime ( input_date , dayfirst = None , input_tz = None , output_tz = None ) \u00a4 Convert input to datetime object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) Parameters: Name Type Description Default input_date input data of any possible format required input_tz input timezone, defaults to utc None output_tz output timezone, defaults to utc None Returns: Type Description datetime.datetime converted datetime format Source code in dietbox/data/wrangling/datetime.py def convert_to_datetime ( input_date , dayfirst = None , input_tz = None , output_tz = None ): \"\"\" Convert input to *datetime* object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int >>> handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) >>> handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) :param input_date: input data of any possible format :param input_tz: input timezone, defaults to utc :param output_tz: output timezone, defaults to utc :return: converted datetime format :rtype: datetime.datetime \"\"\" if dayfirst is None : dayfirst = True if input_tz is None : input_tz = datetime . timezone . utc if output_tz is None : output_tz = datetime . timezone . utc res = None if isinstance ( input_date , datetime . datetime ): res = input_date if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) elif isinstance ( input_date , str ): try : res = dateutil . parser . parse ( input_date , dayfirst = dayfirst ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass elif isinstance ( input_date , ( float , int )): try : res = datetime . datetime . utcfromtimestamp ( input_date / 1000 ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass else : raise Exception ( \"Could not convert {} to datetime: type {} is not handled\" . format ( input_date , type ( input_date ) ) ) return res date_range_has_weekday ( dt_start , dt_end ) \u00a4 date_range_has_weekday decides if the given date range contains weekday Parameters: Name Type Description Default dt_start datetime of the start of date range required dt_end datetime of the end of date range required Source code in dietbox/data/wrangling/datetime.py def date_range_has_weekday ( dt_start , dt_end ): \"\"\" date_range_has_weekday decides if the given date range contains weekday :param dt_start: datetime of the start of date range :param dt_end: datetime of the end of date range \"\"\" res = [] if pd . isnull ( dt_start ) or pd . isnull ( dt_end ): logger . warning ( f \"date start end not specified: { dt_start } , { dt_end } \" ) return None if isinstance ( dt_start , str ): dt_start = pd . to_datetime ( dt_start ) if isinstance ( dt_end , str ): dt_end = pd . to_datetime ( dt_end ) for dt in pd . date_range ( dt_start , dt_end ): if dt . weekday () < 5 : res . append ( True ) else : res . append ( False ) return True in res unpack_datetime ( data ) \u00a4 unpack_datetime converts datetime (string) to a dict of useful date information Source code in dietbox/data/wrangling/datetime.py def unpack_datetime ( data ): \"\"\" unpack_datetime converts datetime (string) to a dict of useful date information \"\"\" res = {} dt = convert_to_datetime ( data , dayfirst = False ) if dt : try : res [ \"year\" ] = dt . year except Exception as e : logger . error ( f \"Could not find year for { dt } (raw: { data } )\" ) try : res [ \"month\" ] = dt . month except Exception as e : logger . error ( f \"Could not find month for { dt } (raw: { data } )\" ) try : res [ \"day\" ] = dt . day except Exception as e : logger . error ( f \"Could not find day for { dt } (raw: { data } )\" ) try : res [ \"weekday\" ] = dt . weekday () + 1 except Exception as e : logger . error ( f \"Could not find weekday for { dt } (raw: { data } )\" ) return res","title":"data.wrangling.datetime"},{"location":"references/data/wrangling/datetime/#data-wrangling-datetime","text":"","title":"Data - Wrangling - datetime"},{"location":"references/data/wrangling/datetime/#dietbox.data.wrangling.datetime.convert_to_datetime","text":"Convert input to datetime object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) Parameters: Name Type Description Default input_date input data of any possible format required input_tz input timezone, defaults to utc None output_tz output timezone, defaults to utc None Returns: Type Description datetime.datetime converted datetime format Source code in dietbox/data/wrangling/datetime.py def convert_to_datetime ( input_date , dayfirst = None , input_tz = None , output_tz = None ): \"\"\" Convert input to *datetime* object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int >>> handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) >>> handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) :param input_date: input data of any possible format :param input_tz: input timezone, defaults to utc :param output_tz: output timezone, defaults to utc :return: converted datetime format :rtype: datetime.datetime \"\"\" if dayfirst is None : dayfirst = True if input_tz is None : input_tz = datetime . timezone . utc if output_tz is None : output_tz = datetime . timezone . utc res = None if isinstance ( input_date , datetime . datetime ): res = input_date if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) elif isinstance ( input_date , str ): try : res = dateutil . parser . parse ( input_date , dayfirst = dayfirst ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass elif isinstance ( input_date , ( float , int )): try : res = datetime . datetime . utcfromtimestamp ( input_date / 1000 ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass else : raise Exception ( \"Could not convert {} to datetime: type {} is not handled\" . format ( input_date , type ( input_date ) ) ) return res","title":"convert_to_datetime()"},{"location":"references/data/wrangling/datetime/#dietbox.data.wrangling.datetime.date_range_has_weekday","text":"date_range_has_weekday decides if the given date range contains weekday Parameters: Name Type Description Default dt_start datetime of the start of date range required dt_end datetime of the end of date range required Source code in dietbox/data/wrangling/datetime.py def date_range_has_weekday ( dt_start , dt_end ): \"\"\" date_range_has_weekday decides if the given date range contains weekday :param dt_start: datetime of the start of date range :param dt_end: datetime of the end of date range \"\"\" res = [] if pd . isnull ( dt_start ) or pd . isnull ( dt_end ): logger . warning ( f \"date start end not specified: { dt_start } , { dt_end } \" ) return None if isinstance ( dt_start , str ): dt_start = pd . to_datetime ( dt_start ) if isinstance ( dt_end , str ): dt_end = pd . to_datetime ( dt_end ) for dt in pd . date_range ( dt_start , dt_end ): if dt . weekday () < 5 : res . append ( True ) else : res . append ( False ) return True in res","title":"date_range_has_weekday()"},{"location":"references/data/wrangling/datetime/#dietbox.data.wrangling.datetime.unpack_datetime","text":"unpack_datetime converts datetime (string) to a dict of useful date information Source code in dietbox/data/wrangling/datetime.py def unpack_datetime ( data ): \"\"\" unpack_datetime converts datetime (string) to a dict of useful date information \"\"\" res = {} dt = convert_to_datetime ( data , dayfirst = False ) if dt : try : res [ \"year\" ] = dt . year except Exception as e : logger . error ( f \"Could not find year for { dt } (raw: { data } )\" ) try : res [ \"month\" ] = dt . month except Exception as e : logger . error ( f \"Could not find month for { dt } (raw: { data } )\" ) try : res [ \"day\" ] = dt . day except Exception as e : logger . error ( f \"Could not find day for { dt } (raw: { data } )\" ) try : res [ \"weekday\" ] = dt . weekday () + 1 except Exception as e : logger . error ( f \"Could not find weekday for { dt } (raw: { data } )\" ) return res","title":"unpack_datetime()"},{"location":"references/data/wrangling/json/","text":"Data - Wrangling - json \u00a4 JSONEncoder ( JSONEncoder ) \u00a4 data serializer for json default ( self , obj ) \u00a4 default serializer Source code in dietbox/data/wrangling/json.py def default ( self , obj ): \"\"\" default serializer \"\"\" if isinstance ( obj , ( datetime . datetime , datetime . date )): return { \"__type__\" : \"__datetime__\" , \"datetime\" : obj . isoformat ()} return json . JSONEncoder . default ( self , obj ) NumpyEncoder ( JSONEncoder ) \u00a4 default ( self , obj ) \u00a4 Implement this method in a subclass such that it returns a serializable object for o , or calls the base implementation (to raise a TypeError ). For example, to support arbitrary iterators, you could implement default like this:: def default(self, o): try: iterable = iter(o) except TypeError: pass else: return list(iterable) return JSONEncoder.default(self, o) Source code in dietbox/data/wrangling/json.py def default ( self , obj ): if isinstance ( obj , np . ndarray ): return obj . tolist () return json . JSONEncoder . default ( self , obj ) decode ( obj ) \u00a4 decode decodes the JSONEncoder results Source code in dietbox/data/wrangling/json.py def decode ( obj ): \"\"\" decode decodes the JSONEncoder results \"\"\" if \"__type__\" in obj : if obj [ \"__type__\" ] == \"__datetime__\" : return dateutil . parser . parse ( obj [ \"datetime\" ]) return obj isoencode ( obj ) \u00a4 isoencode decodes many different objects such as np.bool -> regular bool Source code in dietbox/data/wrangling/json.py def isoencode ( obj ): \"\"\" isoencode decodes many different objects such as np.bool -> regular bool \"\"\" if isinstance ( obj , datetime . datetime ): return obj . isoformat () if isinstance ( obj , datetime . date ): return obj . isoformat () if isinstance ( obj , np . ndarray ): return obj . tolist () if isinstance ( obj , np . int64 ): return int ( obj ) if isinstance ( obj , np . float64 ): return float ( obj ) if isinstance ( obj , np . bool_ ): return bool ( obj )","title":"data.wrangling.json"},{"location":"references/data/wrangling/json/#data-wrangling-json","text":"","title":"Data - Wrangling - json"},{"location":"references/data/wrangling/json/#dietbox.data.wrangling.json.JSONEncoder","text":"data serializer for json","title":"JSONEncoder"},{"location":"references/data/wrangling/json/#dietbox.data.wrangling.json.JSONEncoder.default","text":"default serializer Source code in dietbox/data/wrangling/json.py def default ( self , obj ): \"\"\" default serializer \"\"\" if isinstance ( obj , ( datetime . datetime , datetime . date )): return { \"__type__\" : \"__datetime__\" , \"datetime\" : obj . isoformat ()} return json . JSONEncoder . default ( self , obj )","title":"default()"},{"location":"references/data/wrangling/json/#dietbox.data.wrangling.json.NumpyEncoder","text":"","title":"NumpyEncoder"},{"location":"references/data/wrangling/json/#dietbox.data.wrangling.json.NumpyEncoder.default","text":"Implement this method in a subclass such that it returns a serializable object for o , or calls the base implementation (to raise a TypeError ). For example, to support arbitrary iterators, you could implement default like this:: def default(self, o): try: iterable = iter(o) except TypeError: pass else: return list(iterable) return JSONEncoder.default(self, o) Source code in dietbox/data/wrangling/json.py def default ( self , obj ): if isinstance ( obj , np . ndarray ): return obj . tolist () return json . JSONEncoder . default ( self , obj )","title":"default()"},{"location":"references/data/wrangling/json/#dietbox.data.wrangling.json.decode","text":"decode decodes the JSONEncoder results Source code in dietbox/data/wrangling/json.py def decode ( obj ): \"\"\" decode decodes the JSONEncoder results \"\"\" if \"__type__\" in obj : if obj [ \"__type__\" ] == \"__datetime__\" : return dateutil . parser . parse ( obj [ \"datetime\" ]) return obj","title":"decode()"},{"location":"references/data/wrangling/json/#dietbox.data.wrangling.json.isoencode","text":"isoencode decodes many different objects such as np.bool -> regular bool Source code in dietbox/data/wrangling/json.py def isoencode ( obj ): \"\"\" isoencode decodes many different objects such as np.bool -> regular bool \"\"\" if isinstance ( obj , datetime . datetime ): return obj . isoformat () if isinstance ( obj , datetime . date ): return obj . isoformat () if isinstance ( obj , np . ndarray ): return obj . tolist () if isinstance ( obj , np . int64 ): return int ( obj ) if isinstance ( obj , np . float64 ): return float ( obj ) if isinstance ( obj , np . bool_ ): return bool ( obj )","title":"isoencode()"},{"location":"references/data/wrangling/misc/","text":"Data - Wrangling - misc \u00a4 convert_str_repr_to_list ( inp ) \u00a4 convert_str_repr_to_list concerts string representation of list to list Source code in dietbox/data/wrangling/misc.py def convert_str_repr_to_list ( inp ): \"\"\" convert_str_repr_to_list concerts string representation of list to list \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) elif isinstance ( inp , ( list , tuple , set )): res = list ( inp ) return res convert_str_repr_to_tuple ( inp ) \u00a4 convert_str_repr_to_tuple converts string representation of tuple to tuple Source code in dietbox/data/wrangling/misc.py def convert_str_repr_to_tuple ( inp ): \"\"\" convert_str_repr_to_tuple converts string representation of tuple to tuple \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) if isinstance ( inp , ( list , tuple , set )): res = tuple ( inp ) return res convert_to_bool ( data ) \u00a4 convert_to_bool converts input to bool type in python. The following values are converted to True: 'true' 'yes' '1' 'y' 1 The following values are converted to False: 'false' 'no' '0' 'n' 0 Parameters: Name Type Description Default data input data required Returns: Type Description bool boolean value of the input data Source code in dietbox/data/wrangling/misc.py def convert_to_bool ( data ): \"\"\" convert_to_bool converts input to bool type in python. The following values are converted to True: 1. 'true' 2. 'yes' 3. '1' 4. 'y' 5. 1 The following values are converted to False: 1. 'false' 2. 'no' 3. '0' 4. 'n' 5. 0 :param data: input data :return: boolean value of the input data :rtype: bool \"\"\" res = None if data is None : return res elif isinstance ( data , bool ): res = data elif isinstance ( data , str ): if data . lower () . strip () in [ \"true\" , \"yes\" , \"1\" , \"y\" ]: res = True elif data . lower () . strip () in [ \"false\" , \"no\" , \"0\" , \"n\" ]: res = False else : res = None elif isinstance ( data , ( float , int )): res = bool ( data ) return res convert_to_list ( inp ) \u00a4 convert_to_list converts string representation of lists to list It also works for list, tuple, or set input. Parameters: Name Type Description Default inp string representation of list, list, tuple, set required Returns: Type Description list converted list Source code in dietbox/data/wrangling/misc.py def convert_to_list ( inp ): \"\"\" convert_to_list converts string representation of lists to list It also works for list, tuple, or set input. :param inp: string representation of list, list, tuple, set :return: converted list :rtype: list \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) elif isinstance ( inp , ( list , tuple , set )): res = list ( inp ) return res convert_to_tuple ( inp ) \u00a4 convert_to_tuple converts string representation of tuple to tuple It also works for list, tuple, or set input. Parameters: Name Type Description Default inp string representation of tuple, list, tuple, set required Returns: Type Description tuple converted tuple Source code in dietbox/data/wrangling/misc.py def convert_to_tuple ( inp ): \"\"\" convert_to_tuple converts string representation of tuple to tuple It also works for list, tuple, or set input. :param inp: string representation of tuple, list, tuple, set :return: converted tuple :rtype: tuple \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) if isinstance ( inp , ( list , tuple , set )): res = tuple ( inp ) return res eu_float_string_to_float ( data ) \u00a4 eu_float_string_to_float converts strings in EU format to floats Parameters: Name Type Description Default data str string of the float in EU conventions required Returns: Type Description float converted float from the string Source code in dietbox/data/wrangling/misc.py def eu_float_string_to_float ( data ): \"\"\" eu_float_string_to_float converts strings in EU format to floats :param data: string of the float in EU conventions :type data: str :return: converted float from the string :rtype: float \"\"\" if isinstance ( data , str ): res = data . replace ( \".\" , \"\" ) res = res . replace ( \",\" , \".\" ) try : res = float ( res ) except Exception as e : raise Exception ( f \"Could not convert string { data } to float: { e } \" ) else : raise TypeError ( \"Input data should be string\" ) return res get_value_in_dict_recursively ( dictionary , path , ignore_path_fail = None ) \u00a4 Get value of a dictionary according to specified path (names) Parameters: Name Type Description Default dictionary dict input dictionary required path list path to the value to be obtained This function always returns the value or None. >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} required Source code in dietbox/data/wrangling/misc.py def get_value_in_dict_recursively ( dictionary , path , ignore_path_fail = None ): \"\"\" Get value of a dictionary according to specified path (names) :param dict dictionary: input dictionary :param list path: path to the value to be obtained This function always returns the value or None. >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} \"\"\" if ignore_path_fail is None : ignore_path_fail = True if isinstance ( path , list ): path_temp = path . copy () elif isinstance ( path , tuple ): path_temp = list ( path ) . copy () else : logger . warning ( f \"path is not list or tuple, converting to list: { path } \" ) path_temp = [ path ] . copy () if len ( path_temp ) > 1 : pop = path_temp . pop ( 0 ) try : pop = int ( pop ) except ValueError : if ignore_path_fail : logger . warning ( f \"can not get path\" ) pass else : raise Exception ( f \"specified path ( { path } ) is not acceptable\" ) try : return get_value_in_dict_recursively ( dictionary [ pop ], path_temp ) except : logger . debug ( f \"did not get values for { pop } \" ) return None elif len ( path_temp ) == 0 : return None else : try : val = int ( path_temp [ 0 ]) except : val = path_temp [ 0 ] try : return dictionary [ val ] except KeyError : logger . error ( f \"KeyError: Could not find { path_temp [ 0 ] } \" ) return None except TypeError : logger . error ( f \"TypeError: Could not find { path_temp [ 0 ] } \" ) return None remove_outliers ( dataset , criteria = None ) \u00a4 remove_outliers will filter out the outliers of dataset Changes will be made to original dataset. Parameters: Name Type Description Default dataset dataframe that contains the data to be filtered required Source code in dietbox/data/wrangling/misc.py def remove_outliers ( dataset , criteria = None ): \"\"\" remove_outliers will filter out the outliers of dataset Changes will be made to original dataset. :param dataset: dataframe that contains the data to be filtered \"\"\" logger . info ( \"Removing outliers ... \" ) if criteria is None : criteria = { \"target\" : { \"quantile_range\" : [ 0.01 , 0.99 ]}} for col in criteria : if col not in dataset . columns : logger . warning ( f \"Column { col } is not in dataset ( { dataset . columns } )\" ) continue # Remove isna if required in criteria col_isna = criteria [ col ] . get ( \"isna\" , False ) if col_isna : dataset = dataset . loc [ ~ dataset [ col ] . isna ()] # only use between values col_range = criteria [ col ] . get ( \"range\" , [ - np . inf , np . inf ]) col_quantile_range = criteria [ col ] . get ( \"quantile_range\" , ()) if col_quantile_range : col_range_from_quantile_lower = dataset [ col ] . quantile ( col_quantile_range [ 0 ]) col_range_from_quantile_upper = dataset [ col ] . quantile ( col_quantile_range [ 1 ]) if col_range_from_quantile_lower >= col_range [ 0 ]: col_range [ 0 ] = col_range_from_quantile_lower if col_range_from_quantile_upper <= col_range [ 1 ]: col_range [ 1 ] = col_range_from_quantile_upper dataset = dataset . loc [ dataset [ col ] . between ( * col_range )] logger . info ( \"Removed outliers!\" ) update_dict_recursively ( dictionary , key_path , value ) \u00a4 update or insert values to a dictionary recursively. Parameters: Name Type Description Default dictionary dict the dictionary to be inserted into required key_path list the path for the insertion value required item value to be inserted required Returns: Type Description a dictionary with the inserted value >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} Source code in dietbox/data/wrangling/misc.py def update_dict_recursively ( dictionary , key_path , value ): \"\"\" update or insert values to a dictionary recursively. :param dict dictionary: the dictionary to be inserted into :param list key_path: the path for the insertion value :param item: value to be inserted :returns: a dictionary with the inserted value >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} \"\"\" sub_dictionary = dictionary for key in key_path [: - 1 ]: if key not in sub_dictionary : sub_dictionary [ key ] = {} sub_dictionary = sub_dictionary [ key ] sub_dictionary [ key_path [ - 1 ]] = value return dictionary","title":"data.wrangling.misc"},{"location":"references/data/wrangling/misc/#data-wrangling-misc","text":"","title":"Data - Wrangling - misc"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.convert_str_repr_to_list","text":"convert_str_repr_to_list concerts string representation of list to list Source code in dietbox/data/wrangling/misc.py def convert_str_repr_to_list ( inp ): \"\"\" convert_str_repr_to_list concerts string representation of list to list \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) elif isinstance ( inp , ( list , tuple , set )): res = list ( inp ) return res","title":"convert_str_repr_to_list()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.convert_str_repr_to_tuple","text":"convert_str_repr_to_tuple converts string representation of tuple to tuple Source code in dietbox/data/wrangling/misc.py def convert_str_repr_to_tuple ( inp ): \"\"\" convert_str_repr_to_tuple converts string representation of tuple to tuple \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) if isinstance ( inp , ( list , tuple , set )): res = tuple ( inp ) return res","title":"convert_str_repr_to_tuple()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.convert_to_bool","text":"convert_to_bool converts input to bool type in python. The following values are converted to True: 'true' 'yes' '1' 'y' 1 The following values are converted to False: 'false' 'no' '0' 'n' 0 Parameters: Name Type Description Default data input data required Returns: Type Description bool boolean value of the input data Source code in dietbox/data/wrangling/misc.py def convert_to_bool ( data ): \"\"\" convert_to_bool converts input to bool type in python. The following values are converted to True: 1. 'true' 2. 'yes' 3. '1' 4. 'y' 5. 1 The following values are converted to False: 1. 'false' 2. 'no' 3. '0' 4. 'n' 5. 0 :param data: input data :return: boolean value of the input data :rtype: bool \"\"\" res = None if data is None : return res elif isinstance ( data , bool ): res = data elif isinstance ( data , str ): if data . lower () . strip () in [ \"true\" , \"yes\" , \"1\" , \"y\" ]: res = True elif data . lower () . strip () in [ \"false\" , \"no\" , \"0\" , \"n\" ]: res = False else : res = None elif isinstance ( data , ( float , int )): res = bool ( data ) return res","title":"convert_to_bool()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.convert_to_list","text":"convert_to_list converts string representation of lists to list It also works for list, tuple, or set input. Parameters: Name Type Description Default inp string representation of list, list, tuple, set required Returns: Type Description list converted list Source code in dietbox/data/wrangling/misc.py def convert_to_list ( inp ): \"\"\" convert_to_list converts string representation of lists to list It also works for list, tuple, or set input. :param inp: string representation of list, list, tuple, set :return: converted list :rtype: list \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) elif isinstance ( inp , ( list , tuple , set )): res = list ( inp ) return res","title":"convert_to_list()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.convert_to_tuple","text":"convert_to_tuple converts string representation of tuple to tuple It also works for list, tuple, or set input. Parameters: Name Type Description Default inp string representation of tuple, list, tuple, set required Returns: Type Description tuple converted tuple Source code in dietbox/data/wrangling/misc.py def convert_to_tuple ( inp ): \"\"\" convert_to_tuple converts string representation of tuple to tuple It also works for list, tuple, or set input. :param inp: string representation of tuple, list, tuple, set :return: converted tuple :rtype: tuple \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) if isinstance ( inp , ( list , tuple , set )): res = tuple ( inp ) return res","title":"convert_to_tuple()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.eu_float_string_to_float","text":"eu_float_string_to_float converts strings in EU format to floats Parameters: Name Type Description Default data str string of the float in EU conventions required Returns: Type Description float converted float from the string Source code in dietbox/data/wrangling/misc.py def eu_float_string_to_float ( data ): \"\"\" eu_float_string_to_float converts strings in EU format to floats :param data: string of the float in EU conventions :type data: str :return: converted float from the string :rtype: float \"\"\" if isinstance ( data , str ): res = data . replace ( \".\" , \"\" ) res = res . replace ( \",\" , \".\" ) try : res = float ( res ) except Exception as e : raise Exception ( f \"Could not convert string { data } to float: { e } \" ) else : raise TypeError ( \"Input data should be string\" ) return res","title":"eu_float_string_to_float()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.get_value_in_dict_recursively","text":"Get value of a dictionary according to specified path (names) Parameters: Name Type Description Default dictionary dict input dictionary required path list path to the value to be obtained This function always returns the value or None. >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} required Source code in dietbox/data/wrangling/misc.py def get_value_in_dict_recursively ( dictionary , path , ignore_path_fail = None ): \"\"\" Get value of a dictionary according to specified path (names) :param dict dictionary: input dictionary :param list path: path to the value to be obtained This function always returns the value or None. >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} \"\"\" if ignore_path_fail is None : ignore_path_fail = True if isinstance ( path , list ): path_temp = path . copy () elif isinstance ( path , tuple ): path_temp = list ( path ) . copy () else : logger . warning ( f \"path is not list or tuple, converting to list: { path } \" ) path_temp = [ path ] . copy () if len ( path_temp ) > 1 : pop = path_temp . pop ( 0 ) try : pop = int ( pop ) except ValueError : if ignore_path_fail : logger . warning ( f \"can not get path\" ) pass else : raise Exception ( f \"specified path ( { path } ) is not acceptable\" ) try : return get_value_in_dict_recursively ( dictionary [ pop ], path_temp ) except : logger . debug ( f \"did not get values for { pop } \" ) return None elif len ( path_temp ) == 0 : return None else : try : val = int ( path_temp [ 0 ]) except : val = path_temp [ 0 ] try : return dictionary [ val ] except KeyError : logger . error ( f \"KeyError: Could not find { path_temp [ 0 ] } \" ) return None except TypeError : logger . error ( f \"TypeError: Could not find { path_temp [ 0 ] } \" ) return None","title":"get_value_in_dict_recursively()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.remove_outliers","text":"remove_outliers will filter out the outliers of dataset Changes will be made to original dataset. Parameters: Name Type Description Default dataset dataframe that contains the data to be filtered required Source code in dietbox/data/wrangling/misc.py def remove_outliers ( dataset , criteria = None ): \"\"\" remove_outliers will filter out the outliers of dataset Changes will be made to original dataset. :param dataset: dataframe that contains the data to be filtered \"\"\" logger . info ( \"Removing outliers ... \" ) if criteria is None : criteria = { \"target\" : { \"quantile_range\" : [ 0.01 , 0.99 ]}} for col in criteria : if col not in dataset . columns : logger . warning ( f \"Column { col } is not in dataset ( { dataset . columns } )\" ) continue # Remove isna if required in criteria col_isna = criteria [ col ] . get ( \"isna\" , False ) if col_isna : dataset = dataset . loc [ ~ dataset [ col ] . isna ()] # only use between values col_range = criteria [ col ] . get ( \"range\" , [ - np . inf , np . inf ]) col_quantile_range = criteria [ col ] . get ( \"quantile_range\" , ()) if col_quantile_range : col_range_from_quantile_lower = dataset [ col ] . quantile ( col_quantile_range [ 0 ]) col_range_from_quantile_upper = dataset [ col ] . quantile ( col_quantile_range [ 1 ]) if col_range_from_quantile_lower >= col_range [ 0 ]: col_range [ 0 ] = col_range_from_quantile_lower if col_range_from_quantile_upper <= col_range [ 1 ]: col_range [ 1 ] = col_range_from_quantile_upper dataset = dataset . loc [ dataset [ col ] . between ( * col_range )] logger . info ( \"Removed outliers!\" )","title":"remove_outliers()"},{"location":"references/data/wrangling/misc/#dietbox.data.wrangling.misc.update_dict_recursively","text":"update or insert values to a dictionary recursively. Parameters: Name Type Description Default dictionary dict the dictionary to be inserted into required key_path list the path for the insertion value required item value to be inserted required Returns: Type Description a dictionary with the inserted value >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} Source code in dietbox/data/wrangling/misc.py def update_dict_recursively ( dictionary , key_path , value ): \"\"\" update or insert values to a dictionary recursively. :param dict dictionary: the dictionary to be inserted into :param list key_path: the path for the insertion value :param item: value to be inserted :returns: a dictionary with the inserted value >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} \"\"\" sub_dictionary = dictionary for key in key_path [: - 1 ]: if key not in sub_dictionary : sub_dictionary [ key ] = {} sub_dictionary = sub_dictionary [ key ] sub_dictionary [ key_path [ - 1 ]] = value return dictionary","title":"update_dict_recursively()"},{"location":"references/lab/","text":"Lab \u00a4 Experimental codes are being tested here.","title":"lab"},{"location":"references/lab/#lab","text":"Experimental codes are being tested here.","title":"Lab"},{"location":"references/lab/data/","text":"Lab - Data \u00a4","title":"lab.data"},{"location":"references/lab/data/#lab-data","text":"","title":"Lab - Data"},{"location":"references/lab/data/extraction/","text":"Lab - Data - Extraction \u00a4","title":"lab.data.extraction"},{"location":"references/lab/data/extraction/#lab-data-extraction","text":"","title":"Lab - Data - Extraction"},{"location":"references/lab/data/extraction/postgres/","text":"Lab - Data - Extraction - Postgres \u00a4 local_sql_session ( config ) \u00a4 local_sql_session creates the control flow for a local sql session config = { \"sql_hostname\" : os . getenv ( \"PLATFORM_SQL_URI\" ), # 'sql_hostname' \"sql_username\" : os . getenv ( \"PLATFORM_SQL_USERNAME\" ), # 'sql_username' \"sql_password\" : os . getenv ( \"PLATFORM_SQL_PWD\" ), # 'sql_password' \"sql_main_database\" : os . getenv ( \"PLATFORM_SQL_DB\" ), # 'db_name' \"sql_port\" : int ( os . getenv ( \"PLATFORM_SQL_PORT\" )), \"ssh_host\" : os . getenv ( \"SSH_HOST\" ), #'ssh_hostname' \"ssh_user\" : os . getenv ( \"SSH_USER\" ), #'ssh_username' \"ssh_port\" : 22 , \"ssh_key\" : os . getenv ( \"SSH_KEY\" ), } @local_sql_session ( config ) def main ( session ): q = session . execute ( 'SELECT * from app_prod.shipment LIMIT 10;' ) print ( q . fetchall () ) # or use pandas # session.bind refers to the engine df = pd . read_sql ( query , session . bind ) print ( df . head ()) Parameters: Name Type Description Default config dict configuration dictionary required Source code in dietbox/lab/data/extraction/postgres.py def local_sql_session ( config ): \"\"\" local_sql_session creates the control flow for a local sql session ```python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": os.getenv(\"PLATFORM_SQL_DB\"), # 'db_name' \"sql_port\": int(os.getenv(\"PLATFORM_SQL_PORT\")), \"ssh_host\": os.getenv(\"SSH_HOST\"), #'ssh_hostname' \"ssh_user\": os.getenv(\"SSH_USER\"), #'ssh_username' \"ssh_port\": 22, \"ssh_key\": os.getenv(\"SSH_KEY\"), } @local_sql_session(config) def main(session): q = session.execute('SELECT * from app_prod.shipment LIMIT 10;') print( q.fetchall() ) # or use pandas # session.bind refers to the engine df = pd.read_sql(query, session.bind) print(df.head()) ``` :param config: configuration dictionary :type config: dict \"\"\" def local_sql_session_with_config ( function ): @wraps ( function ) def wrapper ( * args , ** kwargs ): return with_local_sql_session ( function , config , * args , ** kwargs ) return wrapper return local_sql_session_with_config remote_sql_session ( config ) \u00a4 local_sql_session creates the control flow for a remote sql session config = { \"sql_hostname\" : os . getenv ( \"PLATFORM_SQL_URI\" ), # 'sql_hostname' \"sql_username\" : os . getenv ( \"PLATFORM_SQL_USERNAME\" ), # 'sql_username' \"sql_password\" : os . getenv ( \"PLATFORM_SQL_PWD\" ), # 'sql_password' \"sql_main_database\" : os . getenv ( \"PLATFORM_SQL_DB\" ), # 'db_name' \"sql_port\" : int ( os . getenv ( \"PLATFORM_SQL_PORT\" )), \"ssh_host\" : os . getenv ( \"SSH_HOST\" ), #'ssh_hostname' \"ssh_user\" : os . getenv ( \"SSH_USER\" ), #'ssh_username' \"ssh_port\" : 22 , \"ssh_key\" : os . getenv ( \"SSH_KEY\" ), } @remote_sql_session ( config ) def main ( session ): q = session . execute ( 'SELECT * from app_prod.shipment LIMIT 10;' ) print ( q . fetchall () ) # or use pandas # session.bind refers to the engine df = pd . read_sql ( query , session . bind ) print ( df . head ()) Parameters: Name Type Description Default config dict configuration dictionary required Source code in dietbox/lab/data/extraction/postgres.py def remote_sql_session ( config ): \"\"\" local_sql_session creates the control flow for a remote sql session ```python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": os.getenv(\"PLATFORM_SQL_DB\"), # 'db_name' \"sql_port\": int(os.getenv(\"PLATFORM_SQL_PORT\")), \"ssh_host\": os.getenv(\"SSH_HOST\"), #'ssh_hostname' \"ssh_user\": os.getenv(\"SSH_USER\"), #'ssh_username' \"ssh_port\": 22, \"ssh_key\": os.getenv(\"SSH_KEY\"), } @remote_sql_session(config) def main(session): q = session.execute('SELECT * from app_prod.shipment LIMIT 10;') print( q.fetchall() ) # or use pandas # session.bind refers to the engine df = pd.read_sql(query, session.bind) print(df.head()) ``` :param config: configuration dictionary :type config: dict \"\"\" def remote_sql_session_with_config ( function ): @wraps ( function ) def wrapper ( * args , ** kwargs ): return with_remote_sql_session ( function , config , * args , ** kwargs ) return wrapper return remote_sql_session_with_config","title":"lab.data.extraction.postgres"},{"location":"references/lab/data/extraction/postgres/#lab-data-extraction-postgres","text":"","title":"Lab - Data - Extraction - Postgres"},{"location":"references/lab/data/extraction/postgres/#dietbox.lab.data.extraction.postgres.local_sql_session","text":"local_sql_session creates the control flow for a local sql session config = { \"sql_hostname\" : os . getenv ( \"PLATFORM_SQL_URI\" ), # 'sql_hostname' \"sql_username\" : os . getenv ( \"PLATFORM_SQL_USERNAME\" ), # 'sql_username' \"sql_password\" : os . getenv ( \"PLATFORM_SQL_PWD\" ), # 'sql_password' \"sql_main_database\" : os . getenv ( \"PLATFORM_SQL_DB\" ), # 'db_name' \"sql_port\" : int ( os . getenv ( \"PLATFORM_SQL_PORT\" )), \"ssh_host\" : os . getenv ( \"SSH_HOST\" ), #'ssh_hostname' \"ssh_user\" : os . getenv ( \"SSH_USER\" ), #'ssh_username' \"ssh_port\" : 22 , \"ssh_key\" : os . getenv ( \"SSH_KEY\" ), } @local_sql_session ( config ) def main ( session ): q = session . execute ( 'SELECT * from app_prod.shipment LIMIT 10;' ) print ( q . fetchall () ) # or use pandas # session.bind refers to the engine df = pd . read_sql ( query , session . bind ) print ( df . head ()) Parameters: Name Type Description Default config dict configuration dictionary required Source code in dietbox/lab/data/extraction/postgres.py def local_sql_session ( config ): \"\"\" local_sql_session creates the control flow for a local sql session ```python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": os.getenv(\"PLATFORM_SQL_DB\"), # 'db_name' \"sql_port\": int(os.getenv(\"PLATFORM_SQL_PORT\")), \"ssh_host\": os.getenv(\"SSH_HOST\"), #'ssh_hostname' \"ssh_user\": os.getenv(\"SSH_USER\"), #'ssh_username' \"ssh_port\": 22, \"ssh_key\": os.getenv(\"SSH_KEY\"), } @local_sql_session(config) def main(session): q = session.execute('SELECT * from app_prod.shipment LIMIT 10;') print( q.fetchall() ) # or use pandas # session.bind refers to the engine df = pd.read_sql(query, session.bind) print(df.head()) ``` :param config: configuration dictionary :type config: dict \"\"\" def local_sql_session_with_config ( function ): @wraps ( function ) def wrapper ( * args , ** kwargs ): return with_local_sql_session ( function , config , * args , ** kwargs ) return wrapper return local_sql_session_with_config","title":"local_sql_session()"},{"location":"references/lab/data/extraction/postgres/#dietbox.lab.data.extraction.postgres.remote_sql_session","text":"local_sql_session creates the control flow for a remote sql session config = { \"sql_hostname\" : os . getenv ( \"PLATFORM_SQL_URI\" ), # 'sql_hostname' \"sql_username\" : os . getenv ( \"PLATFORM_SQL_USERNAME\" ), # 'sql_username' \"sql_password\" : os . getenv ( \"PLATFORM_SQL_PWD\" ), # 'sql_password' \"sql_main_database\" : os . getenv ( \"PLATFORM_SQL_DB\" ), # 'db_name' \"sql_port\" : int ( os . getenv ( \"PLATFORM_SQL_PORT\" )), \"ssh_host\" : os . getenv ( \"SSH_HOST\" ), #'ssh_hostname' \"ssh_user\" : os . getenv ( \"SSH_USER\" ), #'ssh_username' \"ssh_port\" : 22 , \"ssh_key\" : os . getenv ( \"SSH_KEY\" ), } @remote_sql_session ( config ) def main ( session ): q = session . execute ( 'SELECT * from app_prod.shipment LIMIT 10;' ) print ( q . fetchall () ) # or use pandas # session.bind refers to the engine df = pd . read_sql ( query , session . bind ) print ( df . head ()) Parameters: Name Type Description Default config dict configuration dictionary required Source code in dietbox/lab/data/extraction/postgres.py def remote_sql_session ( config ): \"\"\" local_sql_session creates the control flow for a remote sql session ```python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": os.getenv(\"PLATFORM_SQL_DB\"), # 'db_name' \"sql_port\": int(os.getenv(\"PLATFORM_SQL_PORT\")), \"ssh_host\": os.getenv(\"SSH_HOST\"), #'ssh_hostname' \"ssh_user\": os.getenv(\"SSH_USER\"), #'ssh_username' \"ssh_port\": 22, \"ssh_key\": os.getenv(\"SSH_KEY\"), } @remote_sql_session(config) def main(session): q = session.execute('SELECT * from app_prod.shipment LIMIT 10;') print( q.fetchall() ) # or use pandas # session.bind refers to the engine df = pd.read_sql(query, session.bind) print(df.head()) ``` :param config: configuration dictionary :type config: dict \"\"\" def remote_sql_session_with_config ( function ): @wraps ( function ) def wrapper ( * args , ** kwargs ): return with_remote_sql_session ( function , config , * args , ** kwargs ) return wrapper return remote_sql_session_with_config","title":"remote_sql_session()"},{"location":"references/lab/data/extraction/sql/","text":"Lab - Data - Extraction - SQL \u00a4 load_query ( sql_file ) \u00a4 load_query loads a sql query file as strings. load_query is best combined with query_data to retrieve data from database. Parameters: Name Type Description Default sql_file str path to sql file required Returns: Type Description str string representation of the sql query Source code in dietbox/lab/data/extraction/sql.py def load_query ( sql_file ): \"\"\" load_query loads a sql query file as strings. load_query is best combined with query_data to retrieve data from database. :param sql_file: path to sql file :type sql_file: str :return: string representation of the sql query :rtype: str \"\"\" if not os . path . exists ( sql_file ): raise Exception ( f \"SQL file { sql_file } does not exist!\" ) with open ( sql_file ) as fp : sql = fp . read () if not sql : logger . error ( f \"No content from sql file: { sql_file } \" ) return sql query_data ( queries , config ) \u00a4 query_data queries data from MySQL remote and save the data in a dataframe. Input queries should be a list of dictionaries with name and query as keys. The key name is only used for logs. It is strongly advised to include it. .. code-block:: python queries = [{ \"name\": query_name, \"query\": query_content }] Here is an example of the config param. .. code-block:: python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": 'db_name', # 'db_name' \"sql_port\": 3306, \"ssh_host\": 'a.ssh.host.great_host.com', #'ssh_hostname' \"ssh_user\": 'my_ssh_username', #'ssh_username' \"ssh_port\": 22, \"ssh_key\": ssh_key # } Parameters: Name Type Description Default query str SQL queries arranged in a list of dictionaries with . required Returns: Type Description Union[pandas.c,e.frame.DataFrame] dataframe of the returned data from the query Source code in dietbox/lab/data/extraction/sql.py def query_data ( queries , config ): \"\"\" query_data queries data from MySQL remote and save the data in a dataframe. Input queries should be a list of dictionaries with `name` and `query` as keys. The key `name` is only used for logs. It is strongly advised to include it. .. code-block:: python queries = [{ \"name\": query_name, \"query\": query_content }] Here is an example of the config param. .. code-block:: python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": 'db_name', # 'db_name' \"sql_port\": 3306, \"ssh_host\": 'a.ssh.host.great_host.com', #'ssh_hostname' \"ssh_user\": 'my_ssh_username', #'ssh_username' \"ssh_port\": 22, \"ssh_key\": ssh_key # } :param query: SQL queries arranged in a list of dictionaries with . :type query: str :return: dataframe of the returned data from the query :rtype: pandas.core.frame.DataFrame \"\"\" sql_hostname = config . get ( \"sql_hostname\" ) # 'sql_hostname' sql_username = config . get ( \"sql_username\" ) # 'sql_username' sql_password = config . get ( \"sql_password\" ) # 'sql_password' sql_main_database = config . get ( \"sql_main_database\" ) # 'db_name' sql_port = config . get ( \"sql_port\" ) ssh_host = config . get ( \"ssh_host\" ) #'ssh_hostname' ssh_user = config . get ( \"ssh_user\" ) #'ssh_username' ssh_port = config . get ( \"ssh_port\" ) ssh_key = config . get ( \"ssh_key\" ) if ssh_key is None : logger . warning ( \"config does not include ssh_key \\n Using default in .ssh/id_rsa\" ) home = os . path . expanduser ( \"~\" ) pkeyfilepath = os . path . join ( \".ssh\" , \"id_rsa\" ) ssh_key = paramiko . RSAKey . from_private_key_file ( os . path . join ( home , pkeyfilepath ) ) if isinstance ( ssh_key , str ): logger . warning ( f \"input config has ssh_key as strings: { ssh_key } \\n Converting to key\" ) ssh_key = paramiko . RSAKey . from_private_key_file ( ssh_key ) logger . info ( \"Connecting to Server ... \" ) with SSHTunnelForwarder ( ( ssh_host , ssh_port ), ssh_username = ssh_user , ssh_pkey = ssh_key , remote_bind_address = ( sql_hostname , sql_port ), ) as tunnel : sleep ( 3 ) conn = pymysql . connect ( host = \"127.0.0.1\" , user = sql_username , passwd = sql_password , db = sql_main_database , port = tunnel . local_bind_port , ) logger . info ( \"Connected to the MySQL server\" ) sleep ( 1 ) datasets = [] for query in queries : logger . debug ( query ) query_query = query . get ( \"query\" ) logger . info ( f \"Running query for { query . get ( 'name' ) } \" ) data = pd . read_sql_query ( query_query , conn ) query [ \"data\" ] = data . copy () datasets . append ( query ) sleep ( 10 ) conn . close () return datasets","title":"lab.data.extraction.sql"},{"location":"references/lab/data/extraction/sql/#lab-data-extraction-sql","text":"","title":"Lab - Data - Extraction - SQL"},{"location":"references/lab/data/extraction/sql/#dietbox.lab.data.extraction.sql.load_query","text":"load_query loads a sql query file as strings. load_query is best combined with query_data to retrieve data from database. Parameters: Name Type Description Default sql_file str path to sql file required Returns: Type Description str string representation of the sql query Source code in dietbox/lab/data/extraction/sql.py def load_query ( sql_file ): \"\"\" load_query loads a sql query file as strings. load_query is best combined with query_data to retrieve data from database. :param sql_file: path to sql file :type sql_file: str :return: string representation of the sql query :rtype: str \"\"\" if not os . path . exists ( sql_file ): raise Exception ( f \"SQL file { sql_file } does not exist!\" ) with open ( sql_file ) as fp : sql = fp . read () if not sql : logger . error ( f \"No content from sql file: { sql_file } \" ) return sql","title":"load_query()"},{"location":"references/lab/data/extraction/sql/#dietbox.lab.data.extraction.sql.query_data","text":"query_data queries data from MySQL remote and save the data in a dataframe. Input queries should be a list of dictionaries with name and query as keys. The key name is only used for logs. It is strongly advised to include it. .. code-block:: python queries = [{ \"name\": query_name, \"query\": query_content }] Here is an example of the config param. .. code-block:: python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": 'db_name', # 'db_name' \"sql_port\": 3306, \"ssh_host\": 'a.ssh.host.great_host.com', #'ssh_hostname' \"ssh_user\": 'my_ssh_username', #'ssh_username' \"ssh_port\": 22, \"ssh_key\": ssh_key # } Parameters: Name Type Description Default query str SQL queries arranged in a list of dictionaries with . required Returns: Type Description Union[pandas.c,e.frame.DataFrame] dataframe of the returned data from the query Source code in dietbox/lab/data/extraction/sql.py def query_data ( queries , config ): \"\"\" query_data queries data from MySQL remote and save the data in a dataframe. Input queries should be a list of dictionaries with `name` and `query` as keys. The key `name` is only used for logs. It is strongly advised to include it. .. code-block:: python queries = [{ \"name\": query_name, \"query\": query_content }] Here is an example of the config param. .. code-block:: python config = { \"sql_hostname\": os.getenv(\"PLATFORM_SQL_URI\"), # 'sql_hostname' \"sql_username\": os.getenv(\"PLATFORM_SQL_USERNAME\"), # 'sql_username' \"sql_password\": os.getenv(\"PLATFORM_SQL_PWD\"), # 'sql_password' \"sql_main_database\": 'db_name', # 'db_name' \"sql_port\": 3306, \"ssh_host\": 'a.ssh.host.great_host.com', #'ssh_hostname' \"ssh_user\": 'my_ssh_username', #'ssh_username' \"ssh_port\": 22, \"ssh_key\": ssh_key # } :param query: SQL queries arranged in a list of dictionaries with . :type query: str :return: dataframe of the returned data from the query :rtype: pandas.core.frame.DataFrame \"\"\" sql_hostname = config . get ( \"sql_hostname\" ) # 'sql_hostname' sql_username = config . get ( \"sql_username\" ) # 'sql_username' sql_password = config . get ( \"sql_password\" ) # 'sql_password' sql_main_database = config . get ( \"sql_main_database\" ) # 'db_name' sql_port = config . get ( \"sql_port\" ) ssh_host = config . get ( \"ssh_host\" ) #'ssh_hostname' ssh_user = config . get ( \"ssh_user\" ) #'ssh_username' ssh_port = config . get ( \"ssh_port\" ) ssh_key = config . get ( \"ssh_key\" ) if ssh_key is None : logger . warning ( \"config does not include ssh_key \\n Using default in .ssh/id_rsa\" ) home = os . path . expanduser ( \"~\" ) pkeyfilepath = os . path . join ( \".ssh\" , \"id_rsa\" ) ssh_key = paramiko . RSAKey . from_private_key_file ( os . path . join ( home , pkeyfilepath ) ) if isinstance ( ssh_key , str ): logger . warning ( f \"input config has ssh_key as strings: { ssh_key } \\n Converting to key\" ) ssh_key = paramiko . RSAKey . from_private_key_file ( ssh_key ) logger . info ( \"Connecting to Server ... \" ) with SSHTunnelForwarder ( ( ssh_host , ssh_port ), ssh_username = ssh_user , ssh_pkey = ssh_key , remote_bind_address = ( sql_hostname , sql_port ), ) as tunnel : sleep ( 3 ) conn = pymysql . connect ( host = \"127.0.0.1\" , user = sql_username , passwd = sql_password , db = sql_main_database , port = tunnel . local_bind_port , ) logger . info ( \"Connected to the MySQL server\" ) sleep ( 1 ) datasets = [] for query in queries : logger . debug ( query ) query_query = query . get ( \"query\" ) logger . info ( f \"Running query for { query . get ( 'name' ) } \" ) data = pd . read_sql_query ( query_query , conn ) query [ \"data\" ] = data . copy () datasets . append ( query ) sleep ( 10 ) conn . close () return datasets","title":"query_data()"},{"location":"references/lab/model/","text":"Lab - Model \u00a4","title":"lab.model"},{"location":"references/lab/model/#lab-model","text":"","title":"Lab - Model"},{"location":"references/lab/model/feature/","text":"Lab - Model - Feature \u00a4 MultiColumnCategicalEncoder \u00a4 Labelencoder applied to multiple columns __init__ ( self , encoders = None , columns = None , encode_with = None ) special \u00a4 init initializes the MultiColumnCategicalEncoder with encoder and scaler types. Parameters: Name Type Description Default encoders dict, optional dictionary of encoders to be used on cols, defaults to {} None columns list, optional list of columns to be encoded, defaults to None None encode_with scikit learn categorical encoders to be applied on cols None Source code in dietbox/lab/model/feature.py def __init__ ( self , encoders = None , columns = None , encode_with = None ): \"\"\" __init__ initializes the MultiColumnCategicalEncoder with encoder and scaler types. :param encoders: dictionary of encoders to be used on cols, defaults to {} :type encoders: dict, optional :param columns: list of columns to be encoded, defaults to None :type columns: list, optional :param encode_with: scikit learn categorical encoders to be applied on cols \"\"\" self . columns = columns if encode_with is None : encode_with = LabelEncoder self . encode_with = encode_with if encoders : self . encoders = encoders else : self . encoders = {} transform ( self , X ) \u00a4 Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. Parameters: Name Type Description Default X dataset to be transformed required Returns: Type Description transformed dataset Source code in dietbox/lab/model/feature.py def transform ( self , X ): \"\"\" Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. :param X: dataset to be transformed :return: transformed dataset \"\"\" output = X . copy () if self . columns is not None : for col in self . columns : _le = self . encode_with () if self . encoders . get ( col ): output [ col ] = self . encoders . get ( col ) . transform ( output [ col ]) else : output [ col ] = _le . fit_transform ( output [ col ]) logger . debug ( f \"1. preparing encoder for { col } \" ) self . encoders [ col ] = _le self . check_encoders . append ({ col : _le }) else : for colname , col in output . iteritems (): _le = self . encode_with () if self . encoders . get ( col ): output [ colname ] = self . encoders . get ( colname ) . transform ( col ) else : output [ colname ] = _le . fit_transform ( col ) logger . debug ( f \"2. preparing encoder for { col } \" ) self . encoders [ colname ] = _le self . check_encoders . append ({ colname : _le }) return output MultiColumnScaler \u00a4 Scalers to be applied to multiple columns. __init__ ( self , encoders = None , columns = None , encode_with = None ) special \u00a4 init initializes the MultiColumnScaler with encoder and scaler types. Parameters: Name Type Description Default encoders dict, optional dictionary of encoders to be used on cols, defaults to {} None columns list, optional list of columns to be encoded, defaults to None None encode_with scaler to be applied, defaults to StandardScaler None Source code in dietbox/lab/model/feature.py def __init__ ( self , encoders = None , columns = None , encode_with = None ): \"\"\" __init__ initializes the MultiColumnScaler with encoder and scaler types. :param encoders: dictionary of encoders to be used on cols, defaults to {} :type encoders: dict, optional :param columns: list of columns to be encoded, defaults to None :type columns: list, optional :param encode_with: scaler to be applied, defaults to StandardScaler \"\"\" if encode_with is None : encode_with = StandardScaler self . encode_with = encode_with self . columns = columns # array of column names to encode if encoders : self . encoders = encoders else : self . encoders = {} transform ( self , X ) \u00a4 Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. Parameters: Name Type Description Default X dataset to be transformed required Returns: Type Description transformed dataset Source code in dietbox/lab/model/feature.py def transform ( self , X ): \"\"\" Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. :param X: dataset to be transformed :return: transformed dataset \"\"\" output = X . copy () if self . columns is not None : for col in self . columns : _scaler = self . encode_with () if self . encoders . get ( col ): output [ col ] = self . encoders . get ( col ) . transform ( output [[ col ]]) else : output [ col ] = _scaler . fit_transform ( output [[ col ]]) logger . debug ( f \"1. preparing encoder for { col } \" ) self . encoders [ col ] = _scaler self . check_encoders . append ({ col : _scaler }) else : for colname , col in output . iteritems (): _scaler = self . encode_with () if self . encoders . get ( col ): output [ colname ] = self . encoders . get ( colname ) . transform ( pd . DataFrame ( col ) ) else : output [ colname ] = _scaler . fit_transform ( pd . DataFrame ( col )) logger . debug ( f \"2. preparing encoder for { col } \" ) self . encoders [ colname ] = _scaler self . check_encoders . append ({ colname : _scaler }) return output","title":"lab.model.feature"},{"location":"references/lab/model/feature/#lab-model-feature","text":"","title":"Lab - Model - Feature"},{"location":"references/lab/model/feature/#dietbox.lab.model.feature.MultiColumnCategicalEncoder","text":"Labelencoder applied to multiple columns","title":"MultiColumnCategicalEncoder"},{"location":"references/lab/model/feature/#dietbox.lab.model.feature.MultiColumnCategicalEncoder.__init__","text":"init initializes the MultiColumnCategicalEncoder with encoder and scaler types. Parameters: Name Type Description Default encoders dict, optional dictionary of encoders to be used on cols, defaults to {} None columns list, optional list of columns to be encoded, defaults to None None encode_with scikit learn categorical encoders to be applied on cols None Source code in dietbox/lab/model/feature.py def __init__ ( self , encoders = None , columns = None , encode_with = None ): \"\"\" __init__ initializes the MultiColumnCategicalEncoder with encoder and scaler types. :param encoders: dictionary of encoders to be used on cols, defaults to {} :type encoders: dict, optional :param columns: list of columns to be encoded, defaults to None :type columns: list, optional :param encode_with: scikit learn categorical encoders to be applied on cols \"\"\" self . columns = columns if encode_with is None : encode_with = LabelEncoder self . encode_with = encode_with if encoders : self . encoders = encoders else : self . encoders = {}","title":"__init__()"},{"location":"references/lab/model/feature/#dietbox.lab.model.feature.MultiColumnCategicalEncoder.transform","text":"Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. Parameters: Name Type Description Default X dataset to be transformed required Returns: Type Description transformed dataset Source code in dietbox/lab/model/feature.py def transform ( self , X ): \"\"\" Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. :param X: dataset to be transformed :return: transformed dataset \"\"\" output = X . copy () if self . columns is not None : for col in self . columns : _le = self . encode_with () if self . encoders . get ( col ): output [ col ] = self . encoders . get ( col ) . transform ( output [ col ]) else : output [ col ] = _le . fit_transform ( output [ col ]) logger . debug ( f \"1. preparing encoder for { col } \" ) self . encoders [ col ] = _le self . check_encoders . append ({ col : _le }) else : for colname , col in output . iteritems (): _le = self . encode_with () if self . encoders . get ( col ): output [ colname ] = self . encoders . get ( colname ) . transform ( col ) else : output [ colname ] = _le . fit_transform ( col ) logger . debug ( f \"2. preparing encoder for { col } \" ) self . encoders [ colname ] = _le self . check_encoders . append ({ colname : _le }) return output","title":"transform()"},{"location":"references/lab/model/feature/#dietbox.lab.model.feature.MultiColumnScaler","text":"Scalers to be applied to multiple columns.","title":"MultiColumnScaler"},{"location":"references/lab/model/feature/#dietbox.lab.model.feature.MultiColumnScaler.__init__","text":"init initializes the MultiColumnScaler with encoder and scaler types. Parameters: Name Type Description Default encoders dict, optional dictionary of encoders to be used on cols, defaults to {} None columns list, optional list of columns to be encoded, defaults to None None encode_with scaler to be applied, defaults to StandardScaler None Source code in dietbox/lab/model/feature.py def __init__ ( self , encoders = None , columns = None , encode_with = None ): \"\"\" __init__ initializes the MultiColumnScaler with encoder and scaler types. :param encoders: dictionary of encoders to be used on cols, defaults to {} :type encoders: dict, optional :param columns: list of columns to be encoded, defaults to None :type columns: list, optional :param encode_with: scaler to be applied, defaults to StandardScaler \"\"\" if encode_with is None : encode_with = StandardScaler self . encode_with = encode_with self . columns = columns # array of column names to encode if encoders : self . encoders = encoders else : self . encoders = {}","title":"__init__()"},{"location":"references/lab/model/feature/#dietbox.lab.model.feature.MultiColumnScaler.transform","text":"Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. Parameters: Name Type Description Default X dataset to be transformed required Returns: Type Description transformed dataset Source code in dietbox/lab/model/feature.py def transform ( self , X ): \"\"\" Transforms columns of X specified in self.columns using LabelEncoder(). If no columns specified, transforms all columns in X. :param X: dataset to be transformed :return: transformed dataset \"\"\" output = X . copy () if self . columns is not None : for col in self . columns : _scaler = self . encode_with () if self . encoders . get ( col ): output [ col ] = self . encoders . get ( col ) . transform ( output [[ col ]]) else : output [ col ] = _scaler . fit_transform ( output [[ col ]]) logger . debug ( f \"1. preparing encoder for { col } \" ) self . encoders [ col ] = _scaler self . check_encoders . append ({ col : _scaler }) else : for colname , col in output . iteritems (): _scaler = self . encode_with () if self . encoders . get ( col ): output [ colname ] = self . encoders . get ( colname ) . transform ( pd . DataFrame ( col ) ) else : output [ colname ] = _scaler . fit_transform ( pd . DataFrame ( col )) logger . debug ( f \"2. preparing encoder for { col } \" ) self . encoders [ colname ] = _scaler self . check_encoders . append ({ colname : _scaler }) return output","title":"transform()"},{"location":"references/text/","text":"Texts \u00a4","title":"text.wrangling"},{"location":"references/text/#texts","text":"","title":"Texts"},{"location":"references/text/validator/","text":"Texts - Validator \u00a4 data_validator ( input_str , validate_type ) \u00a4 data_validator is an object that hosts several validators for different types of data such as domain name, email address, etc. The inputs are mostly validated using regex. Parameters: Name Type Description Default input_str str input string to be validated required validate_type str the type of data to be validated against required Source code in dietbox/texts/validator.py def data_validator ( input_str , validate_type ): \"\"\" data_validator is an object that hosts several validators for different types of data such as domain name, email address, etc. The inputs are mostly validated using regex. :param input_str: input string to be validated :type input_str: str :param validate_type: the type of data to be validated against :type validate_type: str \"\"\" regex_validate_types = { \"domain\" : re . compile ( r \"^(?:[a-z0-9]\" # First character of the domain r \"(?:[a-z0-9-_]{0,61}[a-z0-9])?\\.)\" # Sub domain + hostname r \"+[a-z0-9][a-z0-9-_]{0,61}\" # First 61 characters of the gTLD r \"[a-z0-9]$\" # Last character of the gTLD ), \"email\" : re . compile ( r \"^(?:[a-z0-9]\" # First character of the domain r \"(?:[a-z0-9-_]{0,61}[a-z0-9])?\\.)\" # Sub domain + hostname r \"+[a-z0-9][a-z0-9-_]{0,61}\" # First 61 characters of the gTLD r \"[a-z0-9]$\" # Last character of the gTLD ), } if validate_type not in regex_validate_types : raise ValueError ( f \"validate_type ( { validate_type } ) is not defined\" ) else : matched = regex_validate_types . get ( validate_type ) . match ( input_str ) if matched : return True else : return False","title":"text.validator"},{"location":"references/text/validator/#texts-validator","text":"","title":"Texts - Validator"},{"location":"references/text/validator/#dietbox.texts.validator.data_validator","text":"data_validator is an object that hosts several validators for different types of data such as domain name, email address, etc. The inputs are mostly validated using regex. Parameters: Name Type Description Default input_str str input string to be validated required validate_type str the type of data to be validated against required Source code in dietbox/texts/validator.py def data_validator ( input_str , validate_type ): \"\"\" data_validator is an object that hosts several validators for different types of data such as domain name, email address, etc. The inputs are mostly validated using regex. :param input_str: input string to be validated :type input_str: str :param validate_type: the type of data to be validated against :type validate_type: str \"\"\" regex_validate_types = { \"domain\" : re . compile ( r \"^(?:[a-z0-9]\" # First character of the domain r \"(?:[a-z0-9-_]{0,61}[a-z0-9])?\\.)\" # Sub domain + hostname r \"+[a-z0-9][a-z0-9-_]{0,61}\" # First 61 characters of the gTLD r \"[a-z0-9]$\" # Last character of the gTLD ), \"email\" : re . compile ( r \"^(?:[a-z0-9]\" # First character of the domain r \"(?:[a-z0-9-_]{0,61}[a-z0-9])?\\.)\" # Sub domain + hostname r \"+[a-z0-9][a-z0-9-_]{0,61}\" # First 61 characters of the gTLD r \"[a-z0-9]$\" # Last character of the gTLD ), } if validate_type not in regex_validate_types : raise ValueError ( f \"validate_type ( { validate_type } ) is not defined\" ) else : matched = regex_validate_types . get ( validate_type ) . match ( input_str ) if matched : return True else : return False","title":"data_validator()"},{"location":"references/text/words/","text":"Texts - Words \u00a4 fuzzy_group_words ( words_list , fuzz_method = None , threshold = None ) \u00a4 fuzzy_group_words groups similar words into groups Parameters: Name Type Description Default words_list list a list of words required fuzz_method a fuzzywuzzy method, defaults to fuzzywuzzy.partial_ratio None threshold int, optional similarity threshold, defaults to 90 None Returns: Type Description list nested list of the grouped words Source code in dietbox/texts/words.py def fuzzy_group_words ( words_list , fuzz_method = None , threshold = None ): \"\"\" fuzzy_group_words groups similar words into groups :param words_list: a list of words :type words_list: list :param fuzz_method: a fuzzywuzzy method, defaults to `fuzzywuzzy.partial_ratio` :param threshold: similarity threshold, defaults to 90 :type threshold: int, optional :return: nested list of the grouped words :rtype: list \"\"\" if fuzz_method is None : fuzz_method = fuzz . partial_ratio if threshold is None : threshold = 90 words_fuzzy_group = [] for i in words_list : words_fuzzy_group_i = [ i ] for j in words_list : fuzz_score = fuzz_method ( i , j ) if ( fuzz_score >= threshold ) & ( j not in words_fuzzy_group_i ): words_fuzzy_group_i . append ( j ) words_fuzzy_group . append ( words_fuzzy_group_i ) return words_fuzzy_group","title":"text.words"},{"location":"references/text/words/#texts-words","text":"","title":"Texts - Words"},{"location":"references/text/words/#dietbox.texts.words.fuzzy_group_words","text":"fuzzy_group_words groups similar words into groups Parameters: Name Type Description Default words_list list a list of words required fuzz_method a fuzzywuzzy method, defaults to fuzzywuzzy.partial_ratio None threshold int, optional similarity threshold, defaults to 90 None Returns: Type Description list nested list of the grouped words Source code in dietbox/texts/words.py def fuzzy_group_words ( words_list , fuzz_method = None , threshold = None ): \"\"\" fuzzy_group_words groups similar words into groups :param words_list: a list of words :type words_list: list :param fuzz_method: a fuzzywuzzy method, defaults to `fuzzywuzzy.partial_ratio` :param threshold: similarity threshold, defaults to 90 :type threshold: int, optional :return: nested list of the grouped words :rtype: list \"\"\" if fuzz_method is None : fuzz_method = fuzz . partial_ratio if threshold is None : threshold = 90 words_fuzzy_group = [] for i in words_list : words_fuzzy_group_i = [ i ] for j in words_list : fuzz_score = fuzz_method ( i , j ) if ( fuzz_score >= threshold ) & ( j not in words_fuzzy_group_i ): words_fuzzy_group_i . append ( j ) words_fuzzy_group . append ( words_fuzzy_group_i ) return words_fuzzy_group","title":"fuzzy_group_words()"},{"location":"references/text/wrangling/","text":"Texts - Wrangling \u00a4","title":"text.wrangling"},{"location":"references/text/wrangling/#texts-wrangling","text":"","title":"Texts - Wrangling"},{"location":"references/text/wrangling/companies/","text":"Texts - Wrangling - Companies \u00a4 clean_company_name ( company_name , remove_chars = None , chars_data = None , remove_legal_form = None , legal_form_data = None , company_name_map = None ) \u00a4 Clean company name by removing unnecessary strings. Remove some special characters. Strip the whitespaces. Remove legal forms such as gmbh, e.v. Source code in dietbox/texts/wrangling/companies.py def clean_company_name ( company_name , remove_chars = None , chars_data = None , remove_legal_form = None , legal_form_data = None , company_name_map = None , ): \"\"\"Clean company name by removing unnecessary strings. 1. Remove some special characters. 2. Strip the whitespaces. 3. Remove legal forms such as gmbh, e.v. \"\"\" if not isinstance ( company_name , str ): logger . warning ( \"clean_company_name:: input company is not string: {} \" . format ( company_name ) ) logger . debug ( f \"clean_company_name:: company_name is { company_name } \" ) if legal_form_data : remove_legal_form = True if remove_legal_form is None : remove_legal_form = False if legal_form_data and ( legal_form_data is None ): legal_form_data = _ELF_CODE if chars_data : remove_chars = True if remove_chars is None : remove_chars = False if remove_chars and ( chars_data is None ): chars_data = [ '\"' ] # unquote name to make sure the string do not have url encodings logger . debug ( f \"clean_company_name:: unquoting { company_name } \" ) company_name = _unquote ( company_name ) company_name = company_name . strip () first_three_characters_forbiden = ( \"*\" , \"=\" , \"#\" , \"%\" , \"+\" ) company_name_first_three = company_name [: 3 ] company_name_fourth_to_last = company_name [ 3 :] for i in first_three_characters_forbiden : company_name_first_three = company_name_first_three . replace ( i , \"\" ) company_name = company_name_first_three + company_name_fourth_to_last company_name = company_name . strip () if remove_legal_form : logger . debug ( f \"clean_company_name:: legalform for { company_name } \" ) for elf in legal_form_data : spaced_elf = \" \" + elf if company_name . endswith ( spaced_elf ): logger . debug ( f \"Removing { elf } from company name { company_name } \" ) company_name = company_name [: - len ( spaced_elf )] company_name = company_name . strip () if remove_chars : # remove special characters from company name for char in chars_data : company_name = company_name . replace ( char , \"\" ) # convert to lower case company_name = company_name . lower () logger . debug ( f \"clean_company_name:: company_name_enforcement for { company_name } \" ) # Rename companies using a pre-defined map if company_name_map : logger . debug ( \"company_data::looked through company name enforcement: {} \" . format ( company_name_map ) ) if company_name_map : company_name = company_name_map . get ( \"company\" ) logger . debug ( f \"company_data::company_name_map exists: { company_name } \" ) return company_name","title":"text.wrangling.companies"},{"location":"references/text/wrangling/companies/#texts-wrangling-companies","text":"","title":"Texts - Wrangling - Companies"},{"location":"references/text/wrangling/companies/#dietbox.texts.wrangling.companies.clean_company_name","text":"Clean company name by removing unnecessary strings. Remove some special characters. Strip the whitespaces. Remove legal forms such as gmbh, e.v. Source code in dietbox/texts/wrangling/companies.py def clean_company_name ( company_name , remove_chars = None , chars_data = None , remove_legal_form = None , legal_form_data = None , company_name_map = None , ): \"\"\"Clean company name by removing unnecessary strings. 1. Remove some special characters. 2. Strip the whitespaces. 3. Remove legal forms such as gmbh, e.v. \"\"\" if not isinstance ( company_name , str ): logger . warning ( \"clean_company_name:: input company is not string: {} \" . format ( company_name ) ) logger . debug ( f \"clean_company_name:: company_name is { company_name } \" ) if legal_form_data : remove_legal_form = True if remove_legal_form is None : remove_legal_form = False if legal_form_data and ( legal_form_data is None ): legal_form_data = _ELF_CODE if chars_data : remove_chars = True if remove_chars is None : remove_chars = False if remove_chars and ( chars_data is None ): chars_data = [ '\"' ] # unquote name to make sure the string do not have url encodings logger . debug ( f \"clean_company_name:: unquoting { company_name } \" ) company_name = _unquote ( company_name ) company_name = company_name . strip () first_three_characters_forbiden = ( \"*\" , \"=\" , \"#\" , \"%\" , \"+\" ) company_name_first_three = company_name [: 3 ] company_name_fourth_to_last = company_name [ 3 :] for i in first_three_characters_forbiden : company_name_first_three = company_name_first_three . replace ( i , \"\" ) company_name = company_name_first_three + company_name_fourth_to_last company_name = company_name . strip () if remove_legal_form : logger . debug ( f \"clean_company_name:: legalform for { company_name } \" ) for elf in legal_form_data : spaced_elf = \" \" + elf if company_name . endswith ( spaced_elf ): logger . debug ( f \"Removing { elf } from company name { company_name } \" ) company_name = company_name [: - len ( spaced_elf )] company_name = company_name . strip () if remove_chars : # remove special characters from company name for char in chars_data : company_name = company_name . replace ( char , \"\" ) # convert to lower case company_name = company_name . lower () logger . debug ( f \"clean_company_name:: company_name_enforcement for { company_name } \" ) # Rename companies using a pre-defined map if company_name_map : logger . debug ( \"company_data::looked through company name enforcement: {} \" . format ( company_name_map ) ) if company_name_map : company_name = company_name_map . get ( \"company\" ) logger . debug ( f \"company_data::company_name_map exists: { company_name } \" ) return company_name","title":"clean_company_name()"},{"location":"references/text/wrangling/consts/","text":"Texts - Wrangling - Consts \u00a4","title":"text.wrangling.consts"},{"location":"references/text/wrangling/consts/#texts-wrangling-consts","text":"","title":"Texts - Wrangling - Consts"},{"location":"references/utils/","text":"Utils \u00a4","title":"utils"},{"location":"references/utils/#utils","text":"","title":"Utils"},{"location":"references/utils/io/","text":"Utils - Sugar \u00a4 check_env ( var_list ) \u00a4 check_env checks if the given environment variables exists Parameters: Name Type Description Default var_list list of variables to be checked required Source code in dietbox/utils/sugar.py def check_env ( var_list ): \"\"\" check_env checks if the given environment variables exists :param var_list: list of variables to be checked :type definition: list \"\"\" missing = [] res = {} for envvar in var_list : envvar_val = os . getenv ( envvar ) if envvar_val is None : missing . append ( envvar ) res [ envvar ] = envvar_val if missing : raise Exception ( f \"Missing envs { missing } \" ) return res","title":"utils.io"},{"location":"references/utils/io/#utils-sugar","text":"","title":"Utils - Sugar"},{"location":"references/utils/io/#dietbox.utils.sugar.check_env","text":"check_env checks if the given environment variables exists Parameters: Name Type Description Default var_list list of variables to be checked required Source code in dietbox/utils/sugar.py def check_env ( var_list ): \"\"\" check_env checks if the given environment variables exists :param var_list: list of variables to be checked :type definition: list \"\"\" missing = [] res = {} for envvar in var_list : envvar_val = os . getenv ( envvar ) if envvar_val is None : missing . append ( envvar ) res [ envvar ] = envvar_val if missing : raise Exception ( f \"Missing envs { missing } \" ) return res","title":"check_env()"},{"location":"references/utils/sugar/","text":"Text - Words \u00a4 check_env ( var_list ) \u00a4 check_env checks if the given environment variables exists Parameters: Name Type Description Default var_list list of variables to be checked required Source code in dietbox/utils/sugar.py def check_env ( var_list ): \"\"\" check_env checks if the given environment variables exists :param var_list: list of variables to be checked :type definition: list \"\"\" missing = [] res = {} for envvar in var_list : envvar_val = os . getenv ( envvar ) if envvar_val is None : missing . append ( envvar ) res [ envvar ] = envvar_val if missing : raise Exception ( f \"Missing envs { missing } \" ) return res","title":"utils.sugar"},{"location":"references/utils/sugar/#text-words","text":"","title":"Text - Words"},{"location":"references/utils/sugar/#dietbox.utils.sugar.check_env","text":"check_env checks if the given environment variables exists Parameters: Name Type Description Default var_list list of variables to be checked required Source code in dietbox/utils/sugar.py def check_env ( var_list ): \"\"\" check_env checks if the given environment variables exists :param var_list: list of variables to be checked :type definition: list \"\"\" missing = [] res = {} for envvar in var_list : envvar_val = os . getenv ( envvar ) if envvar_val is None : missing . append ( envvar ) res [ envvar ] = envvar_val if missing : raise Exception ( f \"Missing envs { missing } \" ) return res","title":"check_env()"},{"location":"references/visual/","text":"Visual \u00a4","title":"visual"},{"location":"references/visual/#visual","text":"","title":"Visual"},{"location":"references/visual/eda/","text":"Visual - EDA \u00a4","title":"visual.eda"},{"location":"references/visual/eda/#visual-eda","text":"","title":"Visual - EDA"},{"location":"tutorials/","text":"Tutorials \u00a4","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"}]}